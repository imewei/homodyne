{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start: Homodyne XPCS Analysis\n",
    "\n",
    "This notebook demonstrates the complete homodyne workflow in 10 minutes:\n",
    "\n",
    "1. Load an XPCS dataset from an HDF5 file\n",
    "2. Configure and run NLSQ static-mode fitting\n",
    "3. Visualize the results\n",
    "4. Interpret the fitted parameters\n",
    "\n",
    "**Requirements:** homodyne installed (`uv sync`), an HDF5 data file.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configure matplotlib for inline plotting in VS Code/Jupyter\n# MUST come before importing matplotlib\n%matplotlib inline\n\n# Standard imports\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom IPython.display import display\n\n# Homodyne imports\nimport homodyne\nfrom homodyne.config import ConfigManager\nfrom homodyne.optimization.nlsq import fit_nlsq_jax\nfrom homodyne.utils.logging import get_logger\n\nprint(f\"Homodyne version: {homodyne.__version__}\")\n\n# Check optimization backend availability\nfrom homodyne.optimization import OPTIMIZATION_STATUS\n\nprint(f\"NLSQ available: {OPTIMIZATION_STATUS['nlsq_available']}\")\nprint(f\"CMC available:  {OPTIMIZATION_STATUS['cmc_available']}\")\n\nlogger = get_logger(__name__)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Homodyne is driven by a YAML configuration file. You can generate a template\n",
    "with the CLI:\n",
    "\n",
    "```bash\n",
    "homodyne-config --mode static --output config_quickstart.yaml\n",
    "```\n",
    "\n",
    "Here we create a minimal configuration programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a minimal YAML configuration string\nconfig_yaml = \"\"\"\nanalysis_mode: \"static\"\n\nanalyzer_parameters:\n  dt: 0.1\n\noptimization:\n  method: \"nlsq\"\n  nlsq:\n    anti_degeneracy:\n      per_angle_mode: \"auto\"\n\ninitial_parameters:\n  parameter_names: [D0, alpha, D_offset]\n  values: [1000.0, -0.5, 0.01]\n\"\"\"\n\n# Save the config to a temporary file\nimport os\nimport tempfile\n\nconfig_file = tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".yaml\", delete=False)\nconfig_file.write(config_yaml)\nconfig_file.close()\nprint(f\"Config written to: {config_file.name}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load the configuration\nconfig = ConfigManager(config_file.name)\n\nanalysis_mode = config.config.get(\"analysis_mode\", \"static\")\nprint(f\"Analysis mode: {analysis_mode}\")\nprint(f\"Initial parameters: {config.get_initial_parameters()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading Data\n",
    "\n",
    "The `load_xpcs_data` function reads an HDF5 file and returns a standardized\n",
    "dictionary. We create synthetic data here for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create synthetic XPCS data for demonstration\n# In a real analysis, use: data = load_xpcs_data(config_file.name)\n\n\ndef generate_synthetic_c2(\n    q=0.054,  # Å⁻¹\n    D0=1200.0,  # Å²/s\n    alpha=-0.5,  # sub-diffusion\n    D_offset=0.05,  # Å²/s\n    beta=0.12,  # speckle contrast\n    offset=1.0,  # background\n    n_phi=5,  # number of angles\n    n_t=40,  # time points\n    dt=0.1,  # s\n    noise_level=0.005,\n    seed=42,\n):\n    \"\"\"Generate synthetic two-time correlation data.\"\"\"\n    rng = np.random.default_rng(seed)\n    t = dt * np.arange(n_t)\n    phi = np.linspace(0, 180, n_phi)\n\n    # Precompute D(t) on the time grid and its cumulative trapezoid\n    D_t = D0 * t**alpha + D_offset\n    trap_avg = 0.5 * (D_t[:-1] + D_t[1:])\n    D_cumsum = np.concatenate([[0.0], np.cumsum(trap_avg)])\n\n    # Build C2 matrix for each angle\n    c2 = np.zeros((n_phi, n_t, n_t))\n    for i_phi in range(n_phi):\n        for i_t1 in range(n_t):\n            for i_t2 in range(i_t1, n_t):\n                # Diffusion integral (cumulative trapezoid)\n                D_integral = abs(D_cumsum[i_t2] - D_cumsum[i_t1]) * dt\n                g1_sq = np.exp(-2 * q**2 * D_integral)\n                c2_val = offset + beta * g1_sq\n                c2[i_phi, i_t1, i_t2] = c2_val\n                c2[i_phi, i_t2, i_t1] = c2_val  # symmetrize\n\n    # Add noise\n    c2 += noise_level * rng.standard_normal(c2.shape)\n\n    return {\n        \"c2_exp\": c2,\n        \"t1\": t,\n        \"t2\": t,\n        \"phi_angles_list\": phi,\n        \"wavevector_q_list\": np.array([q]),\n        \"sigma\": noise_level * np.ones_like(c2),\n        \"L\": 5.0e6,  # 500 µm in Å\n        \"dt\": dt,\n    }\n\n\n# Generate data with known parameters\nTRUE_PARAMS = {\"D0\": 1200.0, \"alpha\": -0.5, \"D_offset\": 0.05}\ndata = generate_synthetic_c2(**TRUE_PARAMS)\n\nprint(f\"C2 shape: {data['c2_exp'].shape}  # (n_phi, n_t1, n_t2)\")\nprint(f\"Time points: {len(data['t1'])}\")\nprint(f\"Angles: {data['phi_angles_list']}\")\nprint(f\"q = {data['wavevector_q_list'][0]:.4f} Å⁻¹\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize the Raw Data\n",
    "\n",
    "Before fitting, always inspect the raw C2 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(\n    1, data[\"c2_exp\"].shape[0], figsize=(3 * data[\"c2_exp\"].shape[0], 3)\n)\nif data[\"c2_exp\"].shape[0] == 1:\n    axes = [axes]\n\nphi = data[\"phi_angles_list\"]\nt = data[\"t1\"]\nc2 = data[\"c2_exp\"]\n\nfor i_phi, ax in enumerate(axes):\n    im = ax.pcolormesh(\n        t, t, c2[i_phi].T, cmap=\"hot\", vmin=0.98, vmax=1.12, shading=\"auto\"\n    )\n    ax.set_xlabel(\"t1 (s)\")\n    ax.set_ylabel(\"t2 (s)\")\n    ax.set_title(f\"φ = {phi[i_phi]:.0f}°\")\n    plt.colorbar(im, ax=ax, label=\"C2\")\n\nplt.suptitle(\"Two-Time Correlation Matrix\", y=1.02, fontsize=12)\nplt.tight_layout()\ndisplay(fig)\nplt.close(fig)\n\nprint(\"Interpretation:\")\nprint(\"  - Bright region near diagonal = fast dynamics (short lag)\")\nprint(\"  - Uniform off-diagonal = long-time background (offset)\")\nprint(\"  - Similar patterns for all angles = static mode appropriate\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run NLSQ Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from homodyne.utils.logging import log_phase\n",
    "\n",
    "print(\"Running NLSQ optimization...\")\n",
    "with log_phase(\"NLSQ\"):\n",
    "    result = fit_nlsq_jax(data, config)\n",
    "\n",
    "print(f\"\\nConvergence: {result.convergence_status}\")\n",
    "print(f\"Reduced chi-squared: {result.reduced_chi_squared:.4f}\")\n",
    "print(f\"Iterations: {result.iterations}\")\n",
    "print(f\"Time: {result.execution_time:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inspect Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display fitted vs true parameters\n# Physical parameters come after per-angle scaling parameters\nn_phi = data[\"c2_exp\"].shape[0]\nphys_offset = 2 * n_phi  # Skip n_phi contrasts + n_phi offsets\n\nprint(f\"{'Parameter':<20} {'True':>12} {'Fitted':>12} {'Error':>12} {'Rel. Error':>12}\")\nprint(\"-\" * 70)\n\nparam_names = [\"D0\", \"alpha\", \"D_offset\"]\nfor i, name in enumerate(param_names):\n    true_val = TRUE_PARAMS.get(name, float(\"nan\"))\n    fitted_val = result.parameters[phys_offset + i]\n    fitted_err = result.uncertainties[phys_offset + i]\n\n    if true_val != 0 and not np.isnan(true_val):\n        rel_err = abs(fitted_val - true_val) / abs(true_val) * 100\n        rel_str = f\"{rel_err:.1f}%\"\n    else:\n        rel_str = \"N/A\"\n\n    print(\n        f\"{name:<20} {true_val:>12.4g} {fitted_val:>12.4g} {fitted_err:>12.4g} {rel_str:>12}\"\n    )\n\nprint(f\"\\nFit quality: {result.quality_flag}\")\nprint(f\"chi^2 / dof: {result.reduced_chi_squared:.4f}  (ideal: ~1.0)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Fit Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract anti-diagonal cuts (fixed lag time)\ndef get_lag_cut(c2, t, lag_idx):\n    \"\"\"Get C2 values along anti-diagonal at given lag index.\"\"\"\n    n = c2.shape[0] - lag_idx\n    c2_vals = np.array([c2[k, k + lag_idx] for k in range(n)])\n    t_vals = t[:n]\n    return t_vals, c2_vals\n\n\nfig, axes = plt.subplots(1, 2, figsize=(11, 4))\n\n# Left: experimental data at several lag times\nax = axes[0]\nlags = [1, 5, 15, 30]\ncolors = plt.cm.plasma(np.linspace(0.1, 0.9, len(lags)))\nfor lag, color in zip(lags, colors):\n    t_vals, c2_vals = get_lag_cut(c2[0], t, lag)\n    lag_time = t[lag] - t[0]\n    ax.plot(\n        t_vals,\n        c2_vals,\n        \"o-\",\n        color=color,\n        markersize=3,\n        alpha=0.8,\n        label=f\"lag = {lag_time:.2f} s\",\n    )\nax.set_xlabel(\"t1 (s)\")\nax.set_ylabel(\"C2\")\nax.set_title(\"Experimental C2 (angle 0°)\")\nax.legend(fontsize=8)\nax.set_ylim(0.98, 1.15)\n\n# Right: C2 vs lag time (anti-diagonal)\nax = axes[1]\nlag_indices = range(1, len(t) - 1)\nlag_times = [t[k] - t[0] for k in lag_indices]\nc2_mean_lag = []\nfor lag_idx in lag_indices:\n    _, c2_vals = get_lag_cut(c2[0], t, lag_idx)\n    c2_mean_lag.append(np.mean(c2_vals))\n\nax.semilogx(lag_times, c2_mean_lag, \"ko\", markersize=4, label=\"Experiment\")\nax.axhline(1.0, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Baseline\")\nax.set_xlabel(\"Lag time (s)\")\nax.set_ylabel(\"⟨C2⟩ (averaged over t1)\")\nax.set_title(f\"Decorrelation curve  (chi²_nu = {result.reduced_chi_squared:.3f})\")\nax.legend()\n\nplt.tight_layout()\ndisplay(fig)\nplt.close(fig)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Physical Interpretation\n",
    "\n",
    "Let's interpret the fitted parameters in physical terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "n_phi = data[\"c2_exp\"].shape[0]\nphys_offset = 2 * n_phi\n\nD0_fitted = result.parameters[phys_offset]\nalpha_fitted = result.parameters[phys_offset + 1]\nD0_err = result.uncertainties[phys_offset]\n\nq = data[\"wavevector_q_list\"][0]  # Å⁻¹\n\n# Estimate relaxation time\n# For anomalous diffusion: tau ~ (q^2 D0)^(-1/alpha)\ntau_q = (q**2 * D0_fitted) ** (-1 / alpha_fitted) if alpha_fitted != 0 else float(\"inf\")\n\n# Estimate particle size from Stokes-Einstein (water at 25°C)\nkT = 4.11e-21  # J (at 25°C)\neta_water = 8.9e-4  # Pa·s\nD0_m2s = D0_fitted * 1e-20  # Å²/s → m²/s\nRh_nm = kT / (6 * np.pi * eta_water * D0_m2s) * 1e9  # Rh in nm\n\nprint(\"Physical Interpretation\")\nprint(\"=\" * 40)\nprint(f\"D0 = {D0_fitted:.0f} ± {D0_err:.0f} Å²/s\")\n\nif alpha_fitted < 0:\n    print(f\"alpha = {alpha_fitted:.3f} → sub-diffusion (caged/gel-like motion)\")\nelif alpha_fitted > 0:\n    print(f\"alpha = {alpha_fitted:.3f} → super-diffusion\")\nelse:\n    print(f\"alpha = {alpha_fitted:.3f} → normal Brownian diffusion\")\n\nprint(\"\\nEstimated particle radius (Stokes-Einstein, water 25°C):\")\nprint(f\"  Rh = {Rh_nm:.1f} nm\")\nprint(f\"\\nCharacteristic relaxation time at q = {q:.4f} Å⁻¹:\")\nprint(f\"  tau_q ~ {tau_q:.3f} s\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom pathlib import Path\n\noutput_dir = Path(\"quickstart_results\")\noutput_dir.mkdir(exist_ok=True)\n\nn_phi = data[\"c2_exp\"].shape[0]\nphys_offset = 2 * n_phi\n\nresult_dict = {\n    \"analysis_mode\": config.config.get(\"analysis_mode\", \"static\"),\n    \"convergence_status\": result.convergence_status,\n    \"reduced_chi_squared\": float(result.reduced_chi_squared),\n    \"parameters\": {\n        \"D0\": {\n            \"value\": float(result.parameters[phys_offset]),\n            \"uncertainty\": float(result.uncertainties[phys_offset]),\n        },\n        \"alpha\": {\n            \"value\": float(result.parameters[phys_offset + 1]),\n            \"uncertainty\": float(result.uncertainties[phys_offset + 1]),\n        },\n        \"D_offset\": {\n            \"value\": float(result.parameters[phys_offset + 2]),\n            \"uncertainty\": float(result.uncertainties[phys_offset + 2]),\n        },\n    },\n    \"execution_time_s\": float(result.execution_time),\n}\n\nwith open(output_dir / \"nlsq_result.json\", \"w\") as f:\n    json.dump(result_dict, f, indent=2)\n\nimport numpy as np\n\nnp.savez(\n    output_dir / \"nlsq_arrays.npz\",\n    parameters=result.parameters,\n    uncertainties=result.uncertainties,\n    covariance=result.covariance,\n)\n\nprint(f\"Results saved to {output_dir}/\")\nprint(\"  nlsq_result.json  — parameter estimates and quality metrics\")\nprint(\"  nlsq_arrays.npz   — numerical arrays (covariance matrix, etc.)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "You have completed the quickstart! From here you can:\n",
    "\n",
    "- **Replace the synthetic data** with your real HDF5 file by setting `file_path` in the config\n",
    "- **Try laminar flow mode** if your sample is in a shear cell: `mode: \"laminar_flow\"`\n",
    "- **Run Bayesian analysis** for publication-quality uncertainties: see `02_static_analysis.ipynb`\n",
    "- **Explore the full config** with `homodyne-config --mode static --output my_config.yaml`\n",
    "\n",
    "### Quick reference\n",
    "\n",
    "| Command | Purpose |\n",
    "|---------|----------|\n",
    "| `homodyne --config config.yaml` | Run full analysis |\n",
    "| `homodyne-config --mode static` | Generate config template |\n",
    "| `homodyne-config --validate --input config.yaml` | Check config |\n",
    "| `homodyne-config-xla --show` | Show CPU tuning recommendations |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup temporary config file\n",
    "os.unlink(config_file.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}