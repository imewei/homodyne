# Diff 1: mcmc.py - CMC-only refactor
# Apply with: git apply docs/refactor/diffs/01-mcmc-cmc-only.diff
#
# This diff:
# - Removes automatic NUTS/CMC selection logic
# - Removes standalone _run_standard_nuts() function
# - Removes retry logic from fit_mcmc_jax (moved to coordinator)
# - Removes _evaluate_convergence_thresholds()
# - Simplifies fit_mcmc_jax to always route through CMC

--- a/homodyne/optimization/mcmc.py
+++ b/homodyne/optimization/mcmc.py
@@ -1,25 +1,16 @@
-"""MCMC + JAX: High-Accuracy Bayesian Analysis for Homodyne v2.1
+"""MCMC + JAX: High-Accuracy Bayesian Analysis for Homodyne v3.0
 ================================================================

-NumPyro/BlackJAX-based MCMC sampling for high-precision parameter estimation
-and uncertainty quantification with automatic NUTS/CMC selection.
+NumPyro/BlackJAX-based MCMC sampling for high-precision parameter estimation.
+All MCMC runs through CMC (Consensus Monte Carlo) coordinator.

 Key Features
 ------------
-- **Automatic NUTS/CMC Selection**: Tri-criteria OR logic based on dataset characteristics
-  - Criterion 1 (Parallelism): num_samples >= min_samples_for_cmc (default: 15)
-  - Criterion 2 (Memory): estimated_memory > memory_threshold_pct (default: 30%)
-  - Criterion 3 (Large Dataset): dataset_size > large_dataset_threshold (default: 1M)
-  - Decision: CMC if (Criterion 1 OR Criterion 2 OR Criterion 3), otherwise NUTS
+- **CMC-Only Architecture**: All MCMC routes through CMC coordinator
+  - Single-shard mode (num_shards=1) for small/single-angle datasets
+  - Multi-shard mode for large datasets with parallelization

 - **Configuration-Driven Parameter Management**: All parameters loaded from YAML config
-  - parameter_space: Bounds and prior distributions
-  - initial_values: Starting points for MCMC chains (e.g., from NLSQ results)
-  - Automatic fallback to mid-point of bounds if values not specified
-
-- **Full Posterior Sampling**: NumPyro/BlackJAX NUTS with comprehensive diagnostics
-  - Unified homodyne model: c2_fitted = contrast * c2_theory + offset
-  - Physics-informed priors from ParameterSpace
-  - Convergence diagnostics: R-hat, ESS, acceptance rate
-  - Auto-retry mechanism with different random seeds (max 3 retries)
+
+- **Full Posterior Sampling**: NumPyro/BlackJAX NUTS per-shard

 - **JAX Acceleration**: CPU-only execution with JIT compilation (v2.3.0+)
-  - Single-device NUTS for small datasets (<1M points)
-  - Multi-shard CMC for large datasets or many samples
-  - Hardware-adaptive selection using HardwareConfig
+  - Per-shard NUTS execution in worker processes
+  - Consensus combination for multi-shard results

 Workflow
 --------
@@ -28,63 +19,25 @@ Workflow
 2. Manually copy best-fit results to config YAML: `initial_parameters.values`
 3. Run MCMC with `--method mcmc` (automatic NUTS/CMC selection)
 4. MCMC uses config-loaded values for faster convergence
-
-**Configuration Structure (YAML)**
-```yaml
-optimization:
-  mcmc:
-    min_samples_for_cmc: 15        # Parallelism threshold
-    memory_threshold_pct: 0.30     # Memory threshold (30%)
-    dense_mass_matrix: false       # Diagonal (fast) vs full covariance (accurate)
-
-initial_parameters:
-  parameter_names: [D0, alpha, D_offset]
-  values: [1234.5, 0.567, 12.34]  # From NLSQ results (manual copy)
-
-parameter_space:
-  bounds:
-    D0: {min: 100.0, max: 5000.0}
-    alpha: {min: 0.1, max: 2.0}
-    D_offset: {min: 0.1, max: 100.0}
-  priors:
-    D0: {type: TruncatedNormal, mu: 1000.0, sigma: 500.0}
-    alpha: {type: TruncatedNormal, mu: 1.0, sigma: 0.3}
-    D_offset: {type: TruncatedNormal, mu: 10.0, sigma: 5.0}
-```
-
-MCMC Philosophy
----------------
-- Gold standard for uncertainty quantification
-- Full posterior sampling (not just point estimates)
-- Essential for critical/publication-quality analysis
-- Complements NLSQ for comprehensive Bayesian workflow
-
-Automatic Selection Logic
---------------------------
-**NUTS (Single-Device):**
-- Fast for small datasets (<1M points)
-- Low overhead, single-device execution
-- Selected when: ALL criteria fail (num_samples < 15) AND (memory < 30%) AND (dataset_size <= 1M)
-
-**CMC (Multi-Shard):**
-- Parallelized for CPU cores or large memory requirements
-- ~10-20% overhead but enables unlimited dataset sizes
-- Selected when: (num_samples >= 15) OR (memory >= 30%) OR (dataset_size > 1M)
-
-**Examples:**
-- 50 phi angles (num_samples=50) → CMC (parallelism criterion)
-- 5 phi angles but 10M points (memory>30%) → CMC (memory criterion)
-- 3 phi angles, 3M pooled points → CMC (large dataset criterion, JAX broadcasting protection)
-- 10 phi angles, 100k points (memory<30%) → NUTS (all criteria fail, minimal overhead)
+
+**Single-Shard Behavior:**
+For small datasets or single-angle data, CMC uses num_shards=1.
+This is equivalent to standalone NUTS with ~1-2s pool creation overhead.
 """

 from __future__ import annotations

 import os
 import time
-from typing import Any, Dict, Tuple
+from typing import Any, Dict

 import numpy as np

 from homodyne.utils.logging import get_logger, log_performance

 logger = get_logger(__name__)

@@ -431,17 +384,11 @@ except ImportError:
 # Import extended MCMCResult with CMC support
-# This provides backward compatibility while supporting CMC
 try:
-    from homodyne.optimization.cmc.bypass import evaluate_cmc_bypass
     from homodyne.optimization.cmc.result import MCMCResult

     HAS_CMC_RESULT = True
 except ImportError:
-    # Fallback to original MCMCResult if CMC module not available
     HAS_CMC_RESULT = False
-
-    class MCMCResult:
-        # ... [fallback class unchanged] ...
+    # ... [fallback MCMCResult class unchanged] ...


 # ... [Helper functions _calculate_midpoint_defaults, _prepare_phi_mapping unchanged] ...
@@ -627,8 +574,7 @@ def fit_mcmc_jax(
     q: float = None,
     L: float = None,
     analysis_mode: str = "static",
     parameter_space: ParameterSpace | None = None,
     initial_values: dict[str, float] | None = None,
-    enable_dataset_optimization: bool = True,
-    estimate_noise: bool = False,
-    noise_model: str = "hierarchical",
-    use_simplified_likelihood: bool = True,
     **kwargs,
 ) -> MCMCResult:
-    """High-accuracy Bayesian parameter estimation using MCMC with automatic NUTS/CMC selection.
-
-    Performs full posterior sampling using NumPyro/BlackJAX with the unified homodyne
-    correlation model. Automatically selects between standard NUTS (single-device) and
-    Consensus Monte Carlo (multi-shard parallelization) based on dual-criteria OR logic:
-    (num_samples >= min_samples_for_cmc) OR (estimated_memory > memory_threshold_pct).
-
-    [... extensive docstring truncated for brevity ...]
+    """Bayesian parameter estimation using CMC-based MCMC.
+
+    All MCMC runs through CMC coordinator. For small datasets or single-angle
+    data, CMC uses num_shards=1 which is equivalent to standalone NUTS.
+
+    Parameters
+    ----------
+    data : np.ndarray
+        Experimental correlation data (flattened or 2D array)
+    sigma : np.ndarray, optional
+        Noise standard deviations. If None, estimated as 1% of data.
+    t1, t2 : np.ndarray
+        Time delay arrays
+    phi : np.ndarray
+        Azimuthal angle values
+    q : float
+        Scattering wavevector magnitude [A^-1]
+    L : float
+        Sample-detector distance [A]
+    analysis_mode : str, default "static"
+        Physics model: "static" or "laminar_flow"
+    parameter_space : ParameterSpace, optional
+        Parameter bounds and priors (loaded from config)
+    initial_values : dict[str, float], optional
+        Initial parameter values for MCMC chains
+    **kwargs
+        Additional MCMC config (n_samples, n_warmup, n_chains, etc.)
+
+    Returns
+    -------
+    MCMCResult
+        MCMC result with posterior samples, diagnostics, and CMC metadata.
+
+    Notes
+    -----
+    **Single-Shard Mode:**
+    For single-angle data or small datasets, CMC uses num_shards=1.
+    The single worker runs NUTS on the full dataset.
+    Overhead: ~1-2s for pool creation vs standalone NUTS.
     """
-
     # Validate dependencies
     if not NUMPYRO_AVAILABLE and not BLACKJAX_AVAILABLE:
         raise ImportError(
-            "NumPyro or BlackJAX is required for MCMC optimization. "
-            "Install with: pip install numpyro blackjax",
+            "NumPyro or BlackJAX is required for MCMC optimization."
         )
-
     if not HAS_CORE_MODULES:
         raise ImportError("Core homodyne modules are required for optimization")

     # Validate input data
     _validate_mcmc_data(data, t1, t2, phi, q, L)

-    # =========================================================================
-    # CONFIG-DRIVEN PARAMETER LOADING
-    # =========================================================================
-    # Load parameter_space and initial_values from config if not provided
-    # This implements three-tier priority: CLI args > config file > package defaults
-    # - parameter_space: Bounds and prior distributions (from YAML parameter_space section)
-    # - initial_values: Starting points for MCMC chains (from YAML initial_parameters.values)
-
-    # Extract config dict once from kwargs (avoid duplicate pop)
+    # === CONFIG-DRIVEN PARAMETER LOADING ===
     config_dict = kwargs.pop("config", None)

-    # Resolve stable prior fallback knob (kwargs > config > default False)
-    env_stable_prior = os.environ.get("HOMODYNE_STABLE_PRIOR", "0") == "1"
-    stable_prior_flag = kwargs.get("stable_prior_fallback")
-    if stable_prior_flag is None:
-        if config_dict is not None:
-            stable_prior_flag = (
-                config_dict.get("optimization", {})
-                .get("mcmc", {})
-                .get("stable_prior_fallback", False)
-            )
-        else:
-            stable_prior_flag = False
-
-    kwargs["stable_prior_fallback"] = bool(stable_prior_flag or env_stable_prior)
-
-    # Step 1: Load parameter_space from config if None
-    # Contains parameter bounds and prior distributions for Bayesian sampling
+    # Load parameter_space
     if parameter_space is None:
-        logger.info("Loading parameter_space from config (not provided as argument)")
         try:
             if config_dict is not None:
                 parameter_space = ParameterSpace.from_config(config_dict)
-                logger.info(
-                    f"Loaded parameter_space from config: "
-                    f"model={parameter_space.model_type}, "
-                    f"num_params={len(parameter_space.parameter_names)}"
-                )
             else:
-                logger.info("No config provided, using package default parameter_space")
                 parameter_space = ParameterSpace.from_defaults(analysis_mode)
-
         except Exception as e:
-            logger.warning(
-                f"Failed to load parameter_space from config: {e}. "
-                "Using package defaults."
-            )
+            logger.warning(f"Failed to load parameter_space: {e}. Using defaults.")
             parameter_space = ParameterSpace.from_defaults(analysis_mode)
-    else:
-        logger.info(
-            f"Using provided parameter_space: "
-            f"model={parameter_space.model_type}, "
-            f"num_params={len(parameter_space.parameter_names)}"
-        )

-    # Step 2: Load initial_values from config if None
+    # Load initial_values
     if initial_values is None:
-        logger.info("Loading initial_values from config (not provided as argument)")
         try:
             if config_dict is not None:
                 from homodyne.config.manager import ConfigManager
-
                 config_mgr = ConfigManager(config_override=config_dict)
                 initial_values = config_mgr.get_initial_parameters()
-
-                if initial_values:
-                    logger.info(
-                        f"Loaded initial_values from config: "
-                        f"{list(initial_values.keys())} = "
-                        f"{[f'{v:.4g}' for v in initial_values.values()]}"
-                    )
-                else:
-                    # Calculate mid-point defaults
-                    logger.info(
-                        "Config has null initial_values, calculating mid-point defaults"
-                    )
+                if not initial_values:
                     initial_values = _calculate_midpoint_defaults(parameter_space)
-                    logger.info(
-                        f"Using mid-point defaults: "
-                        f"{list(initial_values.keys())} = "
-                        f"{[f'{v:.4g}' for v in initial_values.values()]}"
-                    )
             else:
-                # No config, calculate mid-point defaults
-                logger.info(
-                    "No config provided, calculating mid-point defaults for initial_values"
-                )
                 initial_values = _calculate_midpoint_defaults(parameter_space)
-                logger.info(
-                    f"Using mid-point defaults: "
-                    f"{list(initial_values.keys())} = "
-                    f"{[f'{v:.4g}' for v in initial_values.values()]}"
-                )
-
         except Exception as e:
-            logger.warning(
-                f"Failed to load initial_values from config: {e}. "
-                "Using mid-point defaults."
-            )
+            logger.warning(f"Failed to load initial_values: {e}. Using defaults.")
             initial_values = _calculate_midpoint_defaults(parameter_space)
-    else:
-        logger.info(
-            f"Using provided initial_values: "
-            f"{list(initial_values.keys())} = "
-            f"{[f'{v:.4g}' for v in initial_values.values()]}"
-        )

-    # Step 3: Validate loaded parameters
-    # Verify initial_values are within parameter_space bounds
+    # Validate initial_values
     if initial_values is not None:
         is_valid, violations = parameter_space.validate_values(initial_values)
         if not is_valid:
-            raise ValueError(
-                f"Initial parameter values violate bounds:\n" + "\n".join(violations)
-            )
-        logger.info(
-            "Initial parameter values validated successfully (all within bounds)"
-        )
+            raise ValueError(f"Initial values violate bounds:\n" + "\n".join(violations))

-    # Get total dataset size (handles both 1D and multi-dimensional arrays)
+    # === ALWAYS USE CMC ===
     dataset_size = data.size if hasattr(data, "size") else len(data)
+    num_samples = len(np.unique(phi)) if phi is not None else 1

-    # Get number of independent samples for NUTS/CMC decision
-    # For multi-dimensional XPCS data (n_phi, n_t1, n_t2), each phi angle is one sample
-    # For 1D flattened data, count unique phi angles from phi parameter
-    if hasattr(data, "shape") and hasattr(data, "ndim"):
-        if data.ndim > 1:
-            # Multi-dimensional: first dimension is number of samples (phi angles)
-            num_samples = data.shape[0]
-        else:
-            # 1D flattened data: count unique phi angles from phi parameter
-            import numpy as np
-
-            num_samples = len(np.unique(phi)) if phi is not None else dataset_size
-    else:
-        # Fallback for non-array data: count unique phi angles if available
-        import numpy as np
-
-        num_samples = len(np.unique(phi)) if phi is not None else dataset_size
-
-    logger.info("Starting MCMC+JAX sampling")
-    logger.info(
-        f"Dataset size: {dataset_size:,} data points ({num_samples} independent samples)"
-    )
+    logger.info("Starting MCMC via CMC coordinator")
+    logger.info(f"Dataset: {dataset_size:,} points, {num_samples} unique phi angles")
     logger.info(f"Analysis mode: {analysis_mode}")

-    # =========================================================================
-    # AUTOMATIC NUTS/CMC SELECTION - DUAL-CRITERIA OR LOGIC
-    # =========================================================================
-    # DELETED: All selection logic removed - always use CMC
-
-    # Step 1: Detect hardware configuration
-    # DELETED: Hardware detection moved to CMC coordinator
-
-    # Step 2: Extract configurable thresholds from kwargs
-    # DELETED: Threshold logic removed
-
-    # Step 3: Automatic NUTS/CMC selection using tri-criteria OR logic
-    # DELETED: Selection logic removed - always CMC
-
-    # Step 4: Log warnings for edge cases
-    # DELETED: Edge case logic removed
-
-    # Step 5: Execute selected method
-    # SIMPLIFIED: Always CMC
-
-    logger.info("=" * 70)
-    logger.info("Executing Consensus Monte Carlo (CMC)")
-    logger.info("=" * 70)
-
+    # Import CMC coordinator
     try:
         from homodyne.optimization.cmc.coordinator import CMCCoordinator
     except ImportError as e:
-        logger.error(f"CMC module not available: {e}")
-        raise ImportError(
-            "CMC module required for method='cmc'. "
-            "Ensure homodyne.optimization.cmc is installed."
-        ) from e
-
-    # Extract CMC configuration
-    cmc_config = kwargs.pop("cmc_config", {})
+        raise ImportError("CMC module required for MCMC.") from e

-    # DELETED: Bypass evaluation removed - always use CMC
-    # bypass_decision = evaluate_cmc_bypass(...)
-
-    # Add MCMC config to cmc_config if not already present
+    # Build CMC configuration
+    cmc_config = kwargs.pop("cmc_config", {})
     if "mcmc" not in cmc_config:
         cmc_config["mcmc"] = _get_mcmc_config(kwargs)

+    # Force single-shard for single-angle data
+    if num_samples == 1:
+        cmc_config.setdefault("cmc", {}).setdefault("sharding", {})["num_shards"] = 1
+        logger.info("Single-angle data: using single-shard mode")
+
     # Create CMC coordinator
     coordinator = CMCCoordinator(cmc_config)

-    # Run CMC pipeline with config-driven parameters
+    # Run CMC pipeline
     result = coordinator.run_cmc(
         data=data,
         t1=t1,
         t2=t2,
         phi=phi,
         q=q,
         L=L,
         analysis_mode=analysis_mode,
         parameter_space=parameter_space,
         initial_values=initial_values,
     )

-    logger.info(f"CMC execution completed. Used {result.num_shards} shards.")
+    logger.info(f"CMC completed: {result.num_shards} shard(s), converged={result.converged}")
     return result


-# DELETED: _run_standard_nuts() function (~500 lines)
-# NUTS execution now happens ONLY inside CMC workers
-
-
 def _validate_mcmc_data(data, t1, t2, phi, q, L):
     """Validate MCMC input data."""
-    if data is None or data.size == 0:
-        raise ValueError("Data cannot be None or empty")
-
-    required_arrays = [t1, t2, phi]
-    array_names = ["t1", "t2", "phi"]
-
-    for arr, name in zip(required_arrays, array_names, strict=False):
-        if arr is None:
-            raise ValueError(f"{name} cannot be None")
-
-    if q is None or L is None:
-        raise ValueError("q and L parameters cannot be None")
-
-
-def _estimate_noise(data: np.ndarray) -> np.ndarray:
-    """Estimate noise from data if not provided."""
-    # Simple noise estimation: assume 1% relative noise
-    noise_level = 0.01 * np.abs(data)
-    # Minimum noise floor
-    min_noise = 0.001
-    return np.maximum(noise_level, min_noise)
+    # [unchanged]
+    ...


 def _get_mcmc_config(kwargs: dict[str, Any]) -> dict[str, Any]:
     """Get MCMC configuration with optimized defaults."""
-    # [Function body unchanged - still needed by CMC coordinator]
+    # [unchanged - still used by CMC]
     ...


-# DELETED: _evaluate_convergence_thresholds() (~70 lines)
-# Convergence evaluation moved to CMC coordinator._basic_validation()
-
-
 # Keep: _create_numpyro_model() - used by CMC workers
 # Keep: _run_numpyro_sampling() - used by CMC workers
 # Keep: _process_posterior_samples() - used by CMC workers
 # Keep: All helper functions used by the above
+# Keep: _build_single_angle_surrogate_settings() - used by workers
