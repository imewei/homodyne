{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLSQ Optimization Example\n",
    "\n",
    "This notebook demonstrates how to use the **NLSQ** package for trust-region nonlinear least squares optimization in Homodyne v2.0+.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. Generate synthetic XPCS data with known ground truth parameters\n",
    "2. Set up configuration and optimization parameters\n",
    "3. Run NLSQ optimization with error recovery\n",
    "4. Visualize optimization results and fit quality\n",
    "5. Understand device configuration (CPU vs GPU)\n",
    "\n",
    "## Requirements\n",
    "\n",
    "```bash\n",
    "pip install homodyne>=2.0 matplotlib numpy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tests.factories.synthetic_data import generate_static_isotropic_dataset\n",
    "\n",
    "# Check if NLSQ is available\n",
    "try:\n",
    "    import nlsq\n",
    "\n",
    "    print(f\"✓ NLSQ version: {nlsq.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"❌ NLSQ not installed. Run: pip install nlsq\")\n",
    "\n",
    "# Check JAX device\n",
    "try:\n",
    "    import jax\n",
    "\n",
    "    devices = jax.devices()\n",
    "    print(f\"✓ JAX devices: {devices}\")\n",
    "    print(f\"  Default device: {devices[0].platform}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ JAX error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Synthetic XPCS Data\n",
    "\n",
    "We'll create a synthetic dataset with known ground truth parameters:\n",
    "- **D₀** = 1000.0 μm²/s (diffusion coefficient)\n",
    "- **α** = 0.5 (anomalous diffusion exponent, α=1 is normal diffusion)\n",
    "- **D_offset** = 10.0 μm²/s (baseline diffusion offset)\n",
    "- **contrast** = 0.5 (scaling parameter)\n",
    "- **offset** = 1.0 (baseline intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data (static isotropic mode)\n",
    "print(\"Generating synthetic XPCS data...\")\n",
    "\n",
    "# Ground truth parameters\n",
    "TRUE_D0 = 1000.0\n",
    "TRUE_ALPHA = 0.5\n",
    "TRUE_D_OFFSET = 10.0\n",
    "TRUE_CONTRAST = 0.5\n",
    "TRUE_OFFSET = 1.0\n",
    "\n",
    "# Generate dataset (10 phi angles, 25x25 time grid = 6,250 data points)\n",
    "synthetic_data = generate_static_isotropic_dataset(\n",
    "    D0=TRUE_D0,\n",
    "    alpha=TRUE_ALPHA,\n",
    "    D_offset=TRUE_D_OFFSET,\n",
    "    contrast=TRUE_CONTRAST,\n",
    "    offset=TRUE_OFFSET,\n",
    "    noise_level=0.03,  # 3% noise\n",
    "    n_phi=10,\n",
    "    n_t1=25,\n",
    "    n_t2=25,\n",
    "    seed=42,  # For reproducibility\n",
    ")\n",
    "\n",
    "print(f\"✓ Generated {10 * 25 * 25:,} data points\")\n",
    "print(f\"  Phi angles: {synthetic_data['phi_values'].shape}\")\n",
    "print(f\"  t1 values: {synthetic_data['t1_values'].shape}\")\n",
    "print(f\"  t2 values: {synthetic_data['t2_values'].shape}\")\n",
    "print(f\"  g2 values: {synthetic_data['g2'].shape}\")\n",
    "print(f\"  g2 errors: {synthetic_data['g2_err'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Visualize the Synthetic Data\n",
    "\n",
    "Let's plot the g2 correlation function to see what we're optimizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot g2 for a few phi angles\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "phi_indices = [0, 3, 6, 9]  # Plot 4 phi angles\n",
    "\n",
    "for idx, phi_idx in enumerate(phi_indices):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Get g2 for this phi angle (first t1 slice)\n",
    "    g2_slice = synthetic_data[\"g2\"][phi_idx, 0, :]\n",
    "    g2_err_slice = synthetic_data[\"g2_err\"][phi_idx, 0, :]\n",
    "    t2_values = synthetic_data[\"t2_values\"]\n",
    "\n",
    "    # Plot with error bars\n",
    "    ax.errorbar(\n",
    "        t2_values,\n",
    "        g2_slice,\n",
    "        yerr=g2_err_slice,\n",
    "        fmt=\"o\",\n",
    "        markersize=4,\n",
    "        alpha=0.6,\n",
    "        label=\"Data with errors\",\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"t2 (delay time)\", fontsize=10)\n",
    "    ax.set_ylabel(\"g2(φ, t1=0, t2)\", fontsize=10)\n",
    "    ax.set_title(f'φ = {synthetic_data[\"phi_values\"][phi_idx]:.2f} rad', fontsize=11)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Synthetic XPCS Data (g2 correlation)\", fontsize=13, y=1.01)\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: g2 decays from ~1.25 (contrast + offset) to ~1.0 (offset) as t2 increases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configure Optimization Parameters\n",
    "\n",
    "Set up the configuration and initial parameter guesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock configuration\n",
    "class MockConfig:\n",
    "    def __init__(self):\n",
    "        self.optimization = {\"lsq\": {\"max_iterations\": 200, \"tolerance\": 1e-6}}\n",
    "\n",
    "\n",
    "config = MockConfig()\n",
    "\n",
    "# Initial parameter guesses (intentionally perturbed from ground truth)\n",
    "# Format: [contrast, offset, D0, alpha, D_offset]\n",
    "initial_params = np.array(\n",
    "    [\n",
    "        0.45,  # contrast (true: 0.5)\n",
    "        0.95,  # offset (true: 1.0)\n",
    "        900.0,  # D0 (true: 1000.0)\n",
    "        0.55,  # alpha (true: 0.5)\n",
    "        12.0,  # D_offset (true: 10.0)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Parameter bounds (lower, upper)\n",
    "bounds = (\n",
    "    np.array([0.0, 0.8, 100.0, 0.3, 1.0]),  # Lower bounds\n",
    "    np.array([1.0, 1.2, 1e5, 1.5, 1000.0]),  # Upper bounds\n",
    ")\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Max iterations: {config.optimization['lsq']['max_iterations']}\")\n",
    "print(f\"  Tolerance: {config.optimization['lsq']['tolerance']}\")\n",
    "print(\"\\nInitial parameters:\")\n",
    "param_names = [\"contrast\", \"offset\", \"D0\", \"alpha\", \"D_offset\"]\n",
    "for name, value, true_value in zip(\n",
    "    param_names,\n",
    "    initial_params,\n",
    "    [TRUE_CONTRAST, TRUE_OFFSET, TRUE_D0, TRUE_ALPHA, TRUE_D_OFFSET],\n",
    "    strict=False,\n",
    "):\n",
    "    error_pct = abs(value - true_value) / true_value * 100\n",
    "    print(\n",
    "        f\"  {name:12s}: {value:10.4f} (true: {true_value:10.4f}, error: {error_pct:5.1f}%)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run NLSQ Optimization\n",
    "\n",
    "Now let's run the optimization and recover the ground truth parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from homodyne.optimization.nlsq_wrapper import NLSQWrapper\n",
    "\n",
    "# Create wrapper with error recovery enabled\n",
    "wrapper = NLSQWrapper(\n",
    "    enable_large_dataset=False,  # Not needed for 6K points\n",
    "    enable_recovery=True,  # Enable automatic retry on failure\n",
    ")\n",
    "\n",
    "print(\"Running NLSQ optimization...\")\n",
    "print(\"(This may take 10-30 seconds depending on your hardware)\\n\")\n",
    "\n",
    "# Run optimization\n",
    "result = wrapper.fit(\n",
    "    data=synthetic_data,\n",
    "    config=config,\n",
    "    initial_params=initial_params,\n",
    "    bounds=bounds,\n",
    "    analysis_mode=\"static_isotropic\",\n",
    ")\n",
    "\n",
    "print(\"✓ Optimization complete!\\n\")\n",
    "\n",
    "# Display results\n",
    "print(\"=\" * 70)\n",
    "print(\"OPTIMIZATION RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Convergence: {result.convergence_status}\")\n",
    "print(f\"Success: {result.success}\")\n",
    "print(f\"Iterations: {result.n_iterations}\")\n",
    "print(f\"Chi-squared: {result.chi_squared:.6f}\")\n",
    "print(f\"Reduced χ²: {result.reduced_chi_squared:.6f}\")\n",
    "print(f\"\\nDevice: {result.device_info.get('device', 'unknown')}\")\n",
    "print(f\"Platform: {result.device_info.get('platform', 'unknown')}\")\n",
    "\n",
    "if result.recovery_actions:\n",
    "    print(f\"\\nRecovery actions taken: {len(result.recovery_actions)}\")\n",
    "    for action in result.recovery_actions:\n",
    "        print(f\"  - {action}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PARAMETER RECOVERY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Parameter':<15} {'Initial':>12} {'Optimized':>12} {'True':>12} {'Error':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "true_values = [TRUE_CONTRAST, TRUE_OFFSET, TRUE_D0, TRUE_ALPHA, TRUE_D_OFFSET]\n",
    "for i, name in enumerate(param_names):\n",
    "    init_val = initial_params[i]\n",
    "    opt_val = result.parameters[i]\n",
    "    true_val = true_values[i]\n",
    "    error_pct = abs(opt_val - true_val) / true_val * 100\n",
    "\n",
    "    print(\n",
    "        f\"{name:<15} {init_val:12.6f} {opt_val:12.6f} {true_val:12.6f} {error_pct:9.2f}%\"\n",
    "    )\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualize Fit Quality\n",
    "\n",
    "Let's compare the optimized fit to the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from homodyne.core.jax_backend import compute_g2_scaled\n",
    "\n",
    "# Compute optimized g2 using recovered parameters\n",
    "optimized_params_dict = {\n",
    "    \"contrast\": result.parameters[0],\n",
    "    \"offset\": result.parameters[1],\n",
    "    \"D0\": result.parameters[2],\n",
    "    \"alpha\": result.parameters[3],\n",
    "    \"D_offset\": result.parameters[4],\n",
    "}\n",
    "\n",
    "# Compute g2 with optimized parameters\n",
    "g2_fit = compute_g2_scaled(\n",
    "    phi=synthetic_data[\"phi_values\"],\n",
    "    t1=synthetic_data[\"t1_values\"],\n",
    "    t2=synthetic_data[\"t2_values\"],\n",
    "    params=optimized_params_dict,\n",
    "    mode=\"static_isotropic\",\n",
    ")\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, phi_idx in enumerate([0, 3, 6, 9]):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    # Data\n",
    "    g2_data = synthetic_data[\"g2\"][phi_idx, 0, :]\n",
    "    g2_err = synthetic_data[\"g2_err\"][phi_idx, 0, :]\n",
    "    t2 = synthetic_data[\"t2_values\"]\n",
    "\n",
    "    # Fit\n",
    "    g2_fit_slice = g2_fit[phi_idx, 0, :]\n",
    "\n",
    "    # Plot\n",
    "    ax.errorbar(\n",
    "        t2,\n",
    "        g2_data,\n",
    "        yerr=g2_err,\n",
    "        fmt=\"o\",\n",
    "        markersize=4,\n",
    "        alpha=0.5,\n",
    "        label=\"Data\",\n",
    "        color=\"blue\",\n",
    "    )\n",
    "    ax.plot(t2, g2_fit_slice, \"-\", linewidth=2, label=\"NLSQ Fit\", color=\"red\")\n",
    "\n",
    "    # Compute residuals\n",
    "    residuals = (g2_data - g2_fit_slice) / g2_err\n",
    "    rms_residual = np.sqrt(np.mean(residuals**2))\n",
    "\n",
    "    ax.set_xlabel(\"t2 (delay time)\", fontsize=10)\n",
    "    ax.set_ylabel(\"g2(φ, t1=0, t2)\", fontsize=10)\n",
    "    ax.set_title(\n",
    "        f'φ = {synthetic_data[\"phi_values\"][phi_idx]:.2f} rad\\nRMS residual: {rms_residual:.3f}',\n",
    "        fontsize=11,\n",
    "    )\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(f\"NLSQ Fit Quality (χ² = {result.chi_squared:.2f})\", fontsize=13, y=1.01)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Overall reduced χ² = {result.reduced_chi_squared:.4f}\")\n",
    "print(\"  (Values near 1.0 indicate excellent fit quality)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Analyze Parameter Uncertainties\n",
    "\n",
    "The covariance matrix provides uncertainty estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parameter uncertainties from covariance matrix\n",
    "param_uncertainties = np.sqrt(np.diag(result.covariance))\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PARAMETER UNCERTAINTIES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Parameter':<15} {'Value':>12} {'Uncertainty':>12} {'Relative':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, name in enumerate(param_names):\n",
    "    value = result.parameters[i]\n",
    "    uncertainty = param_uncertainties[i]\n",
    "    relative_uncertainty = (uncertainty / value) * 100 if value != 0 else np.inf\n",
    "\n",
    "    print(f\"{name:<15} {value:12.6f} {uncertainty:12.6f} {relative_uncertainty:9.2f}%\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  - Small relative uncertainties (<5%) indicate well-constrained parameters\")\n",
    "print(\"  - Large uncertainties suggest the parameter may be poorly constrained by data\")\n",
    "print(\"  - For publication-quality uncertainty quantification, use MCMC sampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### What We've Demonstrated\n",
    "\n",
    "1. ✅ Generated synthetic XPCS data with known ground truth\n",
    "2. ✅ Set up NLSQ optimization with parameter bounds\n",
    "3. ✅ Recovered ground truth parameters within error tolerance\n",
    "4. ✅ Visualized fit quality and residuals\n",
    "5. ✅ Analyzed parameter uncertainties\n",
    "\n",
    "### Key Features of NLSQ Optimization\n",
    "\n",
    "- **Fast**: Converges in seconds for datasets with thousands of points\n",
    "- **Robust**: Automatic error recovery with parameter perturbation\n",
    "- **GPU-accelerated**: Transparent GPU usage via JAX (when available)\n",
    "- **Backward compatible**: Works with existing homodyne configurations\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Real data**: Replace synthetic data with actual XPCS measurements\n",
    "2. **Advanced analysis**: Try laminar flow mode (7 parameters)\n",
    "3. **Uncertainty quantification**: Use MCMC sampling for publication-quality error bars\n",
    "4. **Large datasets**: Enable `enable_large_dataset=True` for >1M points\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **Migration Guide**: [MIGRATION_OPTIMISTIX_TO_NLSQ.md](../docs/MIGRATION_OPTIMISTIX_TO_NLSQ.md)\n",
    "- **Documentation**: [README.md](../README.md)\n",
    "- **NLSQ Package**: [github.com/imewei/NLSQ](https://github.com/imewei/NLSQ)\n",
    "- **Theory Paper**: [He et al. PNAS 2024](https://doi.org/10.1073/pnas.2401162121)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
