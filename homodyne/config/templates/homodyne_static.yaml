# ==============================================================================
# HOMODYNE STATIC DIFFUSION CONFIGURATION TEMPLATE
# ==============================================================================
# Comprehensive production-ready template for static diffusion analysis.
# 3-parameter model: D(t,φ) = D₀·t^α + D_offset
#
# Use for: Equilibrium systems, pure diffusion, anomalous diffusion
# Parameter count: 3 physical + 2 × N_angles scaling = 3 + 2N total
#
# VERSION: 2.0.0
# UPDATED: 2025-10-25
# ==============================================================================

# ==============================================================================
# METADATA (Required)
# ==============================================================================
# Template metadata describing analysis configuration
metadata:
  config_version: "2.0.0"                     # Configuration file format version
  description: "Static diffusion analysis - 3-parameter model for equilibrium systems"
  analysis_mode: "static"                     # Static diffusion mode
  parameter_count: 3                          # D₀, α, D_offset

  # Physics model description
  physics_model: "D(t,φ) = D₀·t^α + D_offset"  # No shear effects (equilibrium)

  # Integration and correction methods
  integration_method: "discrete_numerical"    # Discrete numerical integration for stability
  diagonal_correction: "mandatory"            # Diagonal correction for consistency

  # Template classification
  recommended_use: "Equilibrium XPCS systems with pure diffusion and anomalous diffusion"
  template_type: "production_ready"           # Production-ready template
  complexity: "comprehensive"                 # All features documented

  # Key features of this configuration
  key_features:
    - "3-parameter static diffusion model"
    - "Multi-angle correlation analysis with phi filtering"
    - "All optimization methods (NLSQ, MCMC, Streaming, CMC)"
    - "Discrete numerical integration for anomalous diffusion"
    - "Comprehensive quality control and validation"
    - "Noise estimation for improved fitting"

  # Optional: Generation metadata (filled by homodyne-config)
  generated_at: null                          # ISO timestamp when config generated
  generated_by: null                          # Tool used to generate config

# ==============================================================================
# ANALYSIS MODE (Required)
# ==============================================================================
# Primary analysis mode selection
analysis_mode: "static"                       # Static diffusion (equilibrium)

# ==============================================================================
# ANALYZER PARAMETERS (Required)
# ==============================================================================
# Core physical and instrumental parameters for analysis
analyzer_parameters:
  # Time resolution
  dt: 0.1                                     # Time step between correlation measurements [seconds]

  # Frame range for analysis
  start_frame: 1000                           # Starting frame number (1-indexed)
  end_frame: 2000                             # Ending frame number (inclusive)

  # Scattering parameters
  scattering:
    wavevector_q: 0.0054                      # Wave vector magnitude [Å⁻¹] - sample dependent

  # Geometry parameters (instrumental setup)
  geometry:
    stator_rotor_gap: 2000000                 # Gap between stator and rotor [Å] (200 microns = 2000000 Å)

# ==============================================================================
# ANALYSIS SETTINGS (Required)
# ==============================================================================
# Analysis mode-specific settings and model description
analysis_settings:
  static_mode: true                           # Static diffusion (no flow)

  # Model description for documentation
  model_description:
    type: "static_diffusion"                  # Static diffusion model
    parameters: 3                             # D₀, α, D_offset
    physics: "Equilibrium anomalous diffusion with time-dependent coefficient"

# ==============================================================================
# EXPERIMENTAL DATA (Required)
# ==============================================================================
# Paths to experimental data files and caching configuration
experimental_data:
  # Primary data file (HDF5 format)
  file_path: "./data/sample/experiment.hdf"   # Preferred: Direct path to HDF5 file

  # Legacy format (data_folder_path + data_file_name)
  data_folder_path: "./data/sample/"          # Folder containing data file
  data_file_name: "experiment.hdf"            # HDF5 data file name

  # Phi angles configuration
  phi_angles_path: "./data/sample/"           # Folder containing phi angles list
  phi_angles_file: "phi_angles_list.txt"      # Text file with phi angles (one per line)

  # Caching for performance (saves processed C2 data)
  cache_file_path: "./data/sample/"           # Cache directory
  cache_filename_template: "cached_c2_static_q{wavevector_q:.4f}_frames_{start_frame}_{end_frame}.npz"
  cache_compression: true                     # Enable compression for cache files

  # Data format specifications
  data_type: "float64"                        # Data type for arrays
  file_format: "HDF5"                         # File format (currently only HDF5 supported)
  exchange_key: "exchange"                    # HDF5 group key for exchange data

# ==============================================================================
# PHI ANGLE FILTERING
# ==============================================================================
# Advanced phi angle filtering for optimal scattering analysis
# For static diffusion: typically use angles parallel (0°) and antiparallel (180°)
# to detect anisotropic effects or verify isotropic behavior
phi_filtering:
  enabled: true                               # Enable angle filtering to reduce parameter count

  # Target angle ranges (all angles normalized to [-180°, 180°])
  # Angles are checked with wrap-aware logic (e.g., [170°, -170°] works correctly)
  target_ranges:
    # Range 1: Near 0° (parallel to potential anisotropy axis)
    - min_angle: -10.0                        # Minimum angle [degrees]
      max_angle: 10.0                         # Maximum angle [degrees]
      description: "Parallel to primary axis"

    # Range 2: Near 180° (antiparallel to anisotropy axis)
    - min_angle: 170.0
      max_angle: -170.0                       # Wraps correctly: 170° to -170° = [170°, 190°]
      description: "Antiparallel to primary axis"

    # Optional: Perpendicular angles for comprehensive anisotropic analysis
    # Uncomment if your system shows perpendicular anisotropic effects
    # - min_angle: 85.0
    #   max_angle: 95.0
    #   description: "Perpendicular axis 1"
    # - min_angle: -95.0
    #   max_angle: -85.0
    #   description: "Perpendicular axis 2"

  # Fallback behavior
  fallback_to_all_angles: true                # Use all angles if no angles match target ranges

  # Filtering algorithm
  algorithm: "range_based"                    # Algorithm type (currently only range_based)
  tolerance: 3.0                              # Angular tolerance [degrees] for range matching

  # Quality control for angle selection
  quality_control:
    min_angles_required: 1                    # Minimum number of angles required for analysis
    max_angle_spread: 36.0                    # Maximum spread within a single range [degrees]
    validate_coverage: true                   # Validate that angles adequately cover target ranges
    require_orthogonal_angles: false          # Not required for static (use true for anisotropic analysis)

# ==============================================================================
# INITIAL PARAMETERS (Required)
# ==============================================================================
# Starting parameter values for optimization
#
# PARAMETER COUNT (Static):
# -------------------------
# Physical parameters: 3 [D₀, α, D_offset]
# Per-angle scaling: 2 × N_angles [contrast, offset]
# Total: 3 + 2N (e.g., 3 angles → 9 parameters)
#
initial_parameters:
  # Parameter names to optimize (3 parameters for static mode)
  parameter_names:
    - D0                                      # Diffusion coefficient prefactor
    - alpha                                   # Anomalous diffusion exponent
    - D_offset                                # Baseline diffusion

  # Optional: Initial values (if not provided, uses mid-point of bounds)
  values: null                                # List of 3 floats matching parameter_names order
  # Example: [1000.0, -1.2, 0.0]

  # Optional: Units for documentation
  units:                                      # Parameter units (for reference)
    - "Å²/s"                                  # D0
    - "dimensionless"                         # alpha
    - "Å²/s"                                  # D_offset

  # Optional: Optimize only a subset of parameters (rest held at initial values)
  active_parameters: null                     # List of parameter names to actively optimize
  # Example: ["D0", "alpha"]  # Fix D_offset

  # Optional: Fix specific parameters at given values
  fixed_parameters: null                      # Dict mapping parameter names to fixed values
  # Example: {"D_offset": 0.0}  # Assume no offset

# ==============================================================================
# PARAMETER SPACE (Required)
# ==============================================================================
# Parameter bounds and constraints for static diffusion model
parameter_space:
  model: "static"                             # Static diffusion mode

  bounds:
    # -------------------------------------------------------------------------
    # DIFFUSION PARAMETERS (3 parameters for static mode)
    # -------------------------------------------------------------------------
    - name: D0
      min: 100.0                              # Minimum diffusion coefficient [Å²/s]
      max: 1e5                                # Maximum diffusion coefficient [Å²/s]
      type: TruncatedNormal                   # Bound type for priors
      prior_mu: 1000.0                        # Prior mean (for Bayesian methods)
      prior_sigma: 1000.0                     # Prior std dev (for Bayesian methods)
      unit: "Å²/s"
      # Physical meaning: Controls magnitude of diffusion in D(t) = D₀·t^α + D_offset
      # Typical range: 100-10000 for colloidal systems

    - name: alpha
      min: -10.0                              # Minimum power-law exponent
      max: 10.0                               # Maximum power-law exponent
      type: Normal
      prior_mu: -1.2                          # Prior mean
      prior_sigma: 0.3                        # Prior std dev
      unit: "dimensionless"
      # Physical meaning: Anomalous diffusion exponent
      # 0 = normal diffusion, <0 = subdiffusion, >0 = superdiffusion
      # Typical range: -2 to 2 for equilibrium systems

    - name: D_offset
      min: -100000.0                          # Minimum baseline diffusion [Å²/s]
      max: 100000.0                           # Maximum baseline diffusion [Å²/s]
      type: Normal
      prior_mu: 0.0                           # Prior mean
      prior_sigma: 150.0                      # Prior std dev
      unit: "Å²/s"
      # Physical meaning: Constant diffusion contribution (can be negative)
      # Typical range: -100 to 100 for most systems

  # Optional: Prior distributions for Bayesian methods
  priors: null                                # Advanced prior specifications (rarely needed)

# ==============================================================================
# OPTIMIZATION METHODS
# ==============================================================================
# Optimization method selection and configuration
optimization:
  method: "nlsq"                              # Options: "nlsq" | "mcmc" | "cmc" | "auto"

  # ---------------------------------------------------------------------------
  # NLSQ - Trust-Region Nonlinear Least Squares (Primary Method)
  # ---------------------------------------------------------------------------
  # Fast, deterministic optimization using Levenberg-Marquardt algorithm
  # Automatic strategy selection based on dataset size:
  #   < 1M points     → STANDARD (curve_fit)
  #   1M-10M points   → LARGE (curve_fit_large)
  #   10M-100M points → CHUNKED (curve_fit_large with progress)
  #   > 100M points   → STREAMING (unlimited data with checkpointing)
  nlsq:
    max_iterations: 100                       # Maximum optimization iterations
    tolerance: 1e-8                           # Convergence tolerance
    trust_region_scale: 1.0                   # Trust region scaling factor (0.1-10.0)
    verbose: false                            # Print iteration details

  # ---------------------------------------------------------------------------
  # MCMC - Markov Chain Monte Carlo (Uncertainty Quantification)
  # ---------------------------------------------------------------------------
  # Provides full posterior distributions and uncertainty estimates
  # Uses NumPyro or BlackJAX with NUTS sampler
  # For static mode with 3 parameters: faster convergence than laminar_flow
  mcmc:
    num_warmup: 1000                          # NUTS warmup/adaptation samples
    num_samples: 2000                         # Posterior samples per chain
    num_chains: 4                             # Number of parallel chains (recommended: 4)
    progress_bar: true                        # Show sampling progress
    target_accept_prob: 0.8                   # NUTS target acceptance probability (0.6-0.9)
    max_tree_depth: 10                        # Maximum NUTS tree depth
    backend: "numpyro"                        # Options: "numpyro" | "blackjax"

    # Chain initialization
    init_strategy: "median"                   # Options: "median" | "uniform" | "prior"

    # Diagnostics and quality control
    check_hmc_diagnostics: true               # Verify convergence diagnostics
    min_ess: 400                              # Minimum effective sample size per parameter
    max_rhat: 1.1                             # Maximum R-hat for convergence

  # ---------------------------------------------------------------------------
  # STREAMING - For Large Datasets (> 100M points)
  # ---------------------------------------------------------------------------
  # Constant memory footprint with checkpoint/resume capability
  streaming:
    enable_checkpoints: true                  # Enable HDF5 checkpoint save/resume
    checkpoint_dir: "./checkpoints"           # Directory for checkpoint files
    checkpoint_frequency: 10                  # Save checkpoint every N batches
    resume_from_checkpoint: true              # Auto-detect and resume from latest checkpoint
    keep_last_checkpoints: 3                  # Number of recent checkpoints to keep (older deleted)

    # Fault tolerance and error recovery
    enable_fault_tolerance: true              # Enable numerical validation and recovery
    max_retries_per_batch: 2                  # Maximum retry attempts per failed batch
    min_success_rate: 0.5                     # Minimum batch success rate (0.0-1.0) before failing

    # Batch processing
    batch_size: null                          # Batch size in points (null = auto-detect based on memory)
    adaptive_batching: true                   # Dynamically adjust batch size based on performance

  # ---------------------------------------------------------------------------
  # CMC - Consensus Monte Carlo (Large-Scale Bayesian Inference)
  # ---------------------------------------------------------------------------
  # For datasets > 1M points requiring full Bayesian uncertainty quantification
  # Parallelizes MCMC across data shards and combines subposteriors
  cmc:
    enable: false                             # Options: false | true | "auto" (auto enables for >1M points)
    min_points_for_cmc: 1000000               # Minimum dataset size to trigger CMC
    backend: "jax"                            # Options: "jax" (GPU) | "numpy" (CPU)
    diagonal_correction: true                 # Apply diagonal correction to correlation matrix

    # Data sharding configuration
    sharding:
      strategy: "stratified"                  # Options: "stratified" | "random" | "contiguous"
      num_shards: "auto"                      # Number of shards (int or "auto" for automatic)
      max_points_per_shard: "auto"            # Maximum points per shard (int or "auto")

    # Initialization strategy for inverse mass matrix
    initialization:
      method: "svi"                           # Options: "svi" | "nlsq" | "identity"
      svi_steps: 1000                         # Number of SVI optimization steps
      svi_learning_rate: 0.01                 # Adam learning rate for SVI
      svi_rank: 5                             # Low-rank approximation rank (1-10)
      fallback_to_identity: true              # Use identity matrix if SVI fails

    # Backend configuration for parallel execution
    backend_config:
      name: "auto"                            # Options: "auto" | "pjit" | "multiprocessing" | "pbs" | "slurm"
      enable_checkpoints: true                # Enable checkpoint functionality
      checkpoint_frequency: 1                 # Save checkpoint every N shards
      checkpoint_dir: "./cmc_checkpoints"     # Directory for CMC checkpoint files
      keep_last_checkpoints: 3                # Number of recent checkpoints to keep
      resume_from_checkpoint: true            # Auto-resume from latest checkpoint

    # Subposterior combination method
    combination:
      method: "weighted_gaussian"             # Options: "weighted_gaussian" | "simple_average" | "auto"
      validate_results: true                  # Validate combined posterior quality
      min_success_rate: 0.8                   # Minimum fraction of shards that must converge (0.0-1.0)

    # Per-shard MCMC configuration
    per_shard_mcmc:
      num_warmup: 500                         # Warmup steps per shard
      num_samples: 1000                       # Samples per shard
      num_chains: 2                           # Chains per shard
      subsample_size: "auto"                  # Subsample size (int or "auto" for automatic subsampling)

    # Convergence validation
    validation:
      strict_mode: false                      # Fail if validation criteria not met
      min_per_shard_ess: 100                  # Minimum effective sample size per parameter per shard
      max_per_shard_rhat: 1.2                 # Maximum R-hat per parameter per shard
      max_between_shard_kl: 0.5               # Maximum KL divergence between shard posteriors
      min_success_rate: 0.8                   # Minimum fraction of shards that must converge

# ==============================================================================
# NOISE ESTIMATION (Optional)
# ==============================================================================
# Automatic noise level estimation using hybrid NumPyro approach
# Estimates per-angle or global noise variance to improve fit quality
noise_estimation:
  enabled: false                              # Enable automatic noise estimation
  model: "per_angle"                          # Options: "per_angle" | "global"

  # Adam optimization settings for noise parameter estimation
  adam_config:
    learning_rate: 0.01                       # Adam learning rate
    max_epochs: 500                           # Maximum optimization epochs
    convergence_threshold: 1e-6               # Convergence threshold for loss
    early_stopping: true                      # Enable early stopping for efficiency

  # Posterior sampling for uncertainty quantification
  posterior_samples: 1200                     # Number of posterior samples for noise uncertainty

  # Quality control for noise estimation
  validation:
    check_convergence: true                   # Verify optimization convergence
    reasonable_range: [1e-4, 1.0]             # Expected noise range (bounds for sanity check)
    warn_outliers: true                       # Warn about unusual noise estimates

  # Per-angle model settings
  per_angle:
    min_angles_required: 2                    # Minimum angles needed for per-angle estimation
    validate_coverage: true                   # Ensure proper angle coverage

# ==============================================================================
# PERFORMANCE OPTIMIZATION
# ==============================================================================
# Performance and memory management settings
performance:
  # Strategy override (null = automatic selection based on dataset size)
  strategy_override: null                     # Options: null | "standard" | "large" | "chunked" | "streaming"

  # Memory management
  memory_limit_gb: null                       # Custom memory limit in GB (null = auto-detect)
  enable_progress: true                       # Show progress bars during optimization

  # Memory optimization settings
  memory_optimization:
    enabled: true                             # Enable memory optimization
    max_memory_usage_gb: 6.0                  # Maximum memory usage [GB]
    chunk_size: 8000                          # Data chunk size for processing
    enable_caching: true                      # Enable intelligent caching
    cache_strategy: "adaptive"                # Options: "adaptive" | "aggressive" | "conservative"

  # Computation settings
  computation:
    enable_jit: true                          # Enable JAX JIT compilation
    gpu_acceleration: "auto"                  # Options: "auto" | true | false
    cpu_threads: "auto"                       # Number of CPU threads (int or "auto")
    vectorization_level: "high"               # Options: "low" | "medium" | "high"

  # Device configuration
  device:
    preferred_device: "auto"                  # Options: "auto" | "cpu" | "gpu"
    gpu_memory_fraction: 0.9                  # Fraction of GPU memory to use (0.0-1.0)
    # Note: GPU support requires Linux + CUDA 12.1-12.9

# ==============================================================================
# LOGGING
# ==============================================================================
# Logging configuration for analysis monitoring
logging:
  enabled: true                               # Enable logging
  level: "INFO"                               # Options: "DEBUG" | "INFO" | "WARNING" | "ERROR"

  # Console logging
  console:
    enabled: true                             # Log to console
    level: "INFO"                             # Console log level
    format: "detailed"                        # Options: "simple" | "detailed"
    colors: true                              # Enable colored output
    show_progress: true                       # Show progress indicators

  # File logging
  file:
    enabled: false                            # Log to file
    level: "DEBUG"                            # File log level
    path: "./logs/"                           # Log directory
    filename: "homodyne_static_analysis.log"  # Log file name
    max_size_mb: 10                           # Maximum log file size before rotation
    backup_count: 5                           # Number of backup log files to keep

  # Module-specific logging levels
  modules:
    "homodyne.data.phi_filtering": "INFO"
    "homodyne.data.xpcs_loader": "INFO"
    "homodyne.optimization.nlsq_wrapper": "INFO"
    "jax._src": "WARNING"                     # Suppress JAX internal messages

# ==============================================================================
# QUALITY CONTROL (Optional)
# ==============================================================================
# Advanced quality control for data validation and analysis monitoring
quality_control:
  enabled: false                              # Enable quality control checks

  # Angle-specific quality checks
  angle_quality:
    validate_angle_coverage: true             # Validate angle coverage adequacy
    min_angles_per_range: 1                   # Minimum angles required per target range
    check_angle_distribution: true            # Check angle distribution uniformity

  # Multi-angle data validation
  multi_angle_validation:
    check_correlation_consistency: true       # Check correlation consistency across angles
    validate_angle_dependencies: true         # Validate physical angle dependencies
    detect_anomalous_angles: true             # Detect and warn about anomalous angles

# ==============================================================================
# VISUALIZATION (Optional)
# ==============================================================================
# Plot generation and output configuration
plotting:
  save_plots: true                            # Save plots to output directory
  show_plots: false                           # Display plots interactively
  format: "png"                               # Options: "png" | "pdf" | "svg"
  dpi: 300                                    # Resolution [dots per inch]
  style: "publication"                        # Plot style (matplotlib style name)

  # Rendering mode selection (hybrid plotting system)
  # Controls speed vs quality tradeoff for C2 heatmap plots
  preview_mode: false                         # false = publication quality (matplotlib, slower)
                                              # true = fast preview (Datashader, 5-10x faster)

  # Datashader configuration (used when preview_mode: true)
  datashader:
    canvas_width: 1200                        # Rendering resolution in pixels
    canvas_height: 1200                       # Higher values = more detail, larger files
    gpu_acceleration: true                    # Use GPU if CuPy available

  # Matplotlib configuration (used when preview_mode: false)
  matplotlib:
    interpolation: "bilinear"                 # Options: "none" | "bilinear" | "bicubic"
    use_tight_layout: true                    # Use tight layout for plots
    savefig_kwargs:
      bbox_inches: "tight"
      pad_inches: 0.1

  # Plot types to generate
  correlation_function: true                  # C1(t) and C2(t1,t2) plots
  fit_quality: true                           # Residuals and fit quality plots
  parameter_distributions: true               # Parameter posterior distributions (MCMC only)
  residual_analysis: true                     # Residual analysis plots

  # Angle-specific plots
  angle_coverage: true                        # Show which angles were used
  angle_correlation: true                     # Correlation quality vs angle

# ==============================================================================
# OUTPUT
# ==============================================================================
# Output file configuration
output:
  directory: "./results"                      # Base output directory
  base_directory: "./homodyne_results/"       # Alternative: more descriptive directory name

  # Output formats
  formats:
    hdf5: true                                # Save results in HDF5 format
    json: true                                # Save results in JSON format
    csv: true                                 # Save results in CSV format (tables only)

  # Output organization
  create_subdirs: true                        # Create method-specific subdirectories (nlsq/, mcmc/, cmc/)
  timestamp_dirs: false                       # Add timestamp to directory names

  # Compression
  compress_hdf5: true                         # Enable HDF5 compression (saves disk space)
  compression_level: 6                        # HDF5 compression level (0-9, higher = more compression)

# ==============================================================================
# VALIDATION (Optional)
# ==============================================================================
# Configuration validation settings
validation:
  strict_mode: false                          # Fail on validation warnings
  check_file_existence: true                  # Verify data files exist before analysis
  validate_parameter_ranges: true             # Check parameters are within reasonable physics bounds
  check_mode_compatibility: true              # Verify configuration matches selected analysis mode

  # Angle validation
  angle_validation:
    require_multiple_angles: false            # Not required for static (can analyze single angle)
    min_angle_count: 1                        # Minimum number of angles required
    validate_angle_ranges: true               # Validate angle ranges are physically meaningful

# ==============================================================================
# STATIC DIFFUSION - USAGE NOTES
# ==============================================================================
#
# QUICK START:
# ------------
# 1. Update experimental_data paths to your HDF5 file
# 2. Adjust phi_filtering target_ranges based on your system
# 3. Run: homodyne --config this_file.yaml --method nlsq
# 4. For uncertainties: homodyne --config this_file.yaml --method mcmc
#
# STATIC DIFFUSION PHYSICS:
# -------------------------
# Model: D(t,φ) = D₀·t^α + D_offset
#
# Parameters:
#   D₀: Diffusion coefficient prefactor (magnitude)
#   α: Anomalous diffusion exponent (0=normal, <0=sub, >0=super)
#   D_offset: Baseline/constant diffusion contribution
#
# Typical values:
#   D₀: 100-10000 Å²/s for colloidal systems
#   α: -2 to 2 for equilibrium systems
#   D_offset: Usually small (-100 to 100 Å²/s)
#
# PARAMETER COUNTING:
# -------------------
# Total = 3 physical + 2 × N_angles scaling
# Example with 3 filtered angles: 3 + 2×3 = 9 parameters
#   Physical: [D₀, α, D_offset]
#   Scaling: [contrast₁, offset₁, contrast₂, offset₂, contrast₃, offset₃]
#
# ANGLE FILTERING FOR STATIC:
# ----------------------------
# Recommended ranges:
#   - 0° (parallel): Detect primary anisotropy axis
#   - 180° (antiparallel): Verify symmetric behavior
#   - Optional 90° (perpendicular): Full anisotropic characterization
#
# Use fallback_to_all_angles: true if anisotropy direction unknown
#
# OPTIMIZATION METHOD SELECTION:
# -------------------------------
# NLSQ (Recommended first):
#   - Fast: ~seconds to minutes
#   - Deterministic point estimates
#   - Good for exploration and initial fit
#
# MCMC (For publication):
#   - Slower: ~minutes to hours
#   - Full posterior distributions
#   - Uncertainty quantification
#   - Use after NLSQ to refine uncertainties
#
# CMC (Large datasets > 1M points):
#   - Parallelizes MCMC across shards
#   - Full Bayesian UQ for big data
#   - Requires HPC resources
#
# Streaming (Automatic for >100M points):
#   - Constant memory footprint
#   - Checkpoint/resume capability
#
# DATASET SIZE HANDLING:
# ----------------------
# NLSQ automatic strategy selection:
#   < 1M points     → STANDARD
#   1M-10M points   → LARGE
#   10M-100M points → CHUNKED
#   > 100M points   → STREAMING
#
# PLATFORM SUPPORT:
# -----------------
# CPU: Linux, macOS, Windows (full support)
# GPU: Linux only (CUDA 12.1-12.9)
#
# Installation:
#   CPU: pip install homodyne
#   GPU (Linux): pip install jax[cuda12-local]==0.8.0 jaxlib==0.8.0 && pip install homodyne
#
# VALIDATION:
# -----------
# homodyne-config --validate this_file.yaml
# python -m homodyne.runtime.utils.system_validator --quick
#
# ==============================================================================
# END OF STATIC DIFFUSION TEMPLATE
# ==============================================================================
