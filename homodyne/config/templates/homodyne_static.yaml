# ==============================================================================
# HOMODYNE STATIC DIFFUSION CONFIGURATION TEMPLATE
# ==============================================================================
# Comprehensive production-ready template for static diffusion analysis.
# 3-parameter model: D(t,φ) = D₀·t^α + D_offset
#
# Use for: Equilibrium systems, pure diffusion, anomalous diffusion
# Parameter count: 3 physical + 2 × N_angles scaling = 3 + 2N total
#
# VERSION: 2.4.0
# UPDATED: 2025-11-08
# ==============================================================================

# ==============================================================================
# METADATA (Required)
# ==============================================================================
# Template metadata describing analysis configuration
metadata:
  config_version: "2.4.0"                     # Configuration file format version
  description: "Static diffusion analysis - 3-parameter model (v2.4.0: CPU-only, per-angle mandatory, auto NUTS/CMC)"
  analysis_mode: "static"                     # Static diffusion mode
  parameter_count: 3                          # D₀, α, D_offset

  # Physics model description
  physics_model: "D(t,φ) = D₀·t^α + D_offset"  # No shear effects (equilibrium)

  # Integration and correction methods
  integration_method: "discrete_numerical"    # Discrete numerical integration for stability
  diagonal_correction: "mandatory"            # Diagonal correction for consistency

  # Template classification
  recommended_use: "Equilibrium XPCS systems with pure diffusion and anomalous diffusion"
  template_type: "production_ready"           # Production-ready template
  complexity: "comprehensive"                 # All features documented

  # Key features of this configuration
  key_features:
    - "3-parameter static diffusion model"
    - "CPU-only architecture (v2.3.0) with JAX 0.8.0 - GPU support removed"
    - "Per-angle scaling mandatory (v2.4.0) - physically correct"
    - "Automatic NUTS/CMC selection in MCMC (v2.1.0) - dual criteria logic"
    - "Multi-angle correlation analysis with phi filtering"
    - "Discrete numerical integration for anomalous diffusion"
    - "Comprehensive quality control and validation"
    - "HPC CPU optimization for multi-core systems"

  # Optional: Generation metadata (filled by homodyne-config)
  generated_at: null                          # ISO timestamp when config generated
  generated_by: null                          # Tool used to generate config

# ==============================================================================
# ANALYSIS MODE (Required)
# ==============================================================================
# Primary analysis mode selection
analysis_mode: "static"                       # Static diffusion (equilibrium)

# ==============================================================================
# ANALYZER PARAMETERS (Required)
# ==============================================================================
# Core physical and instrumental parameters for analysis
analyzer_parameters:
  # Time resolution
  dt: 0.1                                     # Time step between correlation measurements [seconds]

  # Frame range for analysis
  start_frame: 1000                           # Starting frame number (1-indexed)
  end_frame: 2000                             # Ending frame number (inclusive)

  # Scattering parameters
  scattering:
    wavevector_q: 0.0054                      # Wave vector magnitude [Å⁻¹] - sample dependent

  # Geometry parameters (instrumental setup)
  geometry:
    stator_rotor_gap: 2000000                 # Gap between stator and rotor [Å] (200 microns = 2000000 Å)

# ==============================================================================
# ANALYSIS SETTINGS (Required)
# ==============================================================================
# Analysis mode-specific settings and model description
analysis_settings:
  static_mode: true                           # Static diffusion (no flow)

  # Model description for documentation
  model_description:
    type: "static_diffusion"                  # Static diffusion model
    parameters: 3                             # D₀, α, D_offset
    physics: "Equilibrium anomalous diffusion with time-dependent coefficient"

# ==============================================================================
# EXPERIMENTAL DATA (Required)
# ==============================================================================
# Paths to experimental data files and caching configuration
experimental_data:
  # Primary data file (HDF5 format)
  file_path: "./data/sample/experiment.hdf"   # Preferred: Direct path to HDF5 file

  # Legacy format (data_folder_path + data_file_name)
  data_folder_path: "./data/sample/"          # Folder containing data file
  data_file_name: "experiment.hdf"            # HDF5 data file name

  # Phi angles configuration
  phi_angles_path: "./data/sample/"           # Folder containing phi angles list
  phi_angles_file: "phi_angles_list.txt"      # Text file with phi angles (one per line)

  # Caching for performance (saves processed C2 data)
  cache_file_path: "./data/sample/"           # Cache directory
  cache_filename_template: "cached_c2_static_q{wavevector_q:.4f}_frames_{start_frame}_{end_frame}.npz"
  cache_compression: true                     # Enable compression for cache files

  # Data format specifications
  data_type: "float64"                        # Data type for arrays
  file_format: "HDF5"                         # File format (currently only HDF5 supported)
  exchange_key: "exchange"                    # HDF5 group key for exchange data

# ==============================================================================
# PHI ANGLE FILTERING
# ==============================================================================
# Advanced phi angle filtering for optimal scattering analysis
# For static diffusion: typically use angles parallel (0°) and antiparallel (180°)
# to detect anisotropic effects or verify isotropic behavior
phi_filtering:
  enabled: true                               # Enable angle filtering to reduce parameter count

  # Target angle ranges (all angles normalized to [-180°, 180°])
  # Angles are checked with wrap-aware logic (e.g., [170°, -170°] works correctly)
  target_ranges:
    # Range 1: Near 0° (parallel to potential anisotropy axis)
    - min_angle: -10.0                        # Minimum angle [degrees]
      max_angle: 10.0                         # Maximum angle [degrees]
      description: "Parallel to primary axis"

    # Range 2: Near 180° (antiparallel to anisotropy axis)
    - min_angle: 170.0
      max_angle: -170.0                       # Wraps correctly: 170° to -170° = [170°, 190°]
      description: "Antiparallel to primary axis"

    # Optional: Perpendicular angles for comprehensive anisotropic analysis
    # Uncomment if your system shows perpendicular anisotropic effects
    # - min_angle: 85.0
    #   max_angle: 95.0
    #   description: "Perpendicular axis 1"
    # - min_angle: -95.0
    #   max_angle: -85.0
    #   description: "Perpendicular axis 2"

  # Fallback behavior
  fallback_to_all_angles: true                # Use all angles if no angles match target ranges

  # Filtering algorithm
  algorithm: "range_based"                    # Algorithm type (currently only range_based)
  tolerance: 3.0                              # Angular tolerance [degrees] for range matching

  # Quality control for angle selection
  quality_control:
    min_angles_required: 1                    # Minimum number of angles required for analysis
    max_angle_spread: 36.0                    # Maximum spread within a single range [degrees]
    validate_coverage: true                   # Validate that angles adequately cover target ranges
    require_orthogonal_angles: false          # Not required for static (use true for anisotropic analysis)

# ==============================================================================
# INITIAL PARAMETERS (Required)
# ==============================================================================
# Starting parameter values for optimization
#
# PARAMETER COUNT (Static):
# -------------------------
# **PER-ANGLE SCALING MANDATORY (v2.4.0)**:
#   - Legacy scalar contrast/offset mode REMOVED
#   - Each scattering angle has unique optical/detector properties
#   - Passing per_angle_scaling=False will raise ValueError
#
# Physical parameters: 3 [D₀, α, D_offset]
# Per-angle scaling: 2 × N_angles [contrast, offset]
# Total: 3 + 2N (e.g., 3 angles → 9 parameters)
#
initial_parameters:
  # Parameter names to optimize (3 parameters for static mode)
  parameter_names:
    - D0                                      # Diffusion coefficient prefactor
    - alpha                                   # Anomalous diffusion exponent
    - D_offset                                # Baseline diffusion

  # Optional: Initial values (if not provided, uses mid-point of bounds)
  values: null                                # List of 3 floats matching parameter_names order
  # Example: [1000.0, -1.2, 0.0]

  # Optional: Units for documentation
  units:                                      # Parameter units (for reference)
    - "Å²/s"                                  # D0
    - "dimensionless"                         # alpha
    - "Å²/s"                                  # D_offset

  # Optional: Optimize only a subset of parameters (rest held at initial values)
  active_parameters: null                     # List of parameter names to actively optimize
  # Example: ["D0", "alpha"]  # Fix D_offset

  # Optional: Fix specific parameters at given values
  fixed_parameters: null                      # Dict mapping parameter names to fixed values
  # Example: {"D_offset": 0.0}  # Assume no offset

# ==============================================================================
# PARAMETER SPACE (Required)
# ==============================================================================
# Parameter bounds and constraints for static diffusion model
parameter_space:
  model: "static"                             # Static diffusion mode

  bounds:
    # -------------------------------------------------------------------------
    # DIFFUSION PARAMETERS (3 parameters for static mode)
    # -------------------------------------------------------------------------
    - name: D0
      min: 100.0                              # Minimum diffusion coefficient [Å²/s]
      max: 1e5                                # Maximum diffusion coefficient [Å²/s]
      type: TruncatedNormal                   # Bound type for priors
      prior_mu: 1000.0                        # Prior mean (for Bayesian methods)
      prior_sigma: 1000.0                     # Prior std dev (for Bayesian methods)
      unit: "Å²/s"
      # Physical meaning: Controls magnitude of diffusion in D(t) = D₀·t^α + D_offset
      # Typical range: 100-10000 for colloidal systems

    - name: alpha
      min: -2.0                               # ✅ FIXED: Physically realistic minimum (was -10.0)
      max: 2.0                                # ✅ FIXED: Physically realistic maximum (was 10.0)
      type: TruncatedNormal                   # ✅ FIXED: Changed from Normal to enforce bounds
      prior_mu: -1.2                          # Prior mean
      prior_sigma: 0.3                        # Prior std dev
      # ✅ Bounds tightened: [-10,10] caused numerical underflow (alpha=-4.96 → theory≈0)
      unit: "dimensionless"
      # Physical meaning: Anomalous diffusion exponent
      # 0 = normal diffusion, <0 = subdiffusion, >0 = superdiffusion
      # Typical range: -2 to 2 for equilibrium systems

    - name: D_offset
      min: -100000.0                          # Minimum baseline diffusion [Å²/s]
      max: 100000.0                           # Maximum baseline diffusion [Å²/s]
      type: TruncatedNormal                   # ✅ FIXED: Changed from Normal to enforce bounds
      prior_mu: 0.0                           # Prior mean
      prior_sigma: 150.0                      # Prior std dev
      unit: "Å²/s"
      # Physical meaning: Constant diffusion contribution (can be negative)
      # Typical range: -100 to 100 for most systems

  # Optional: Prior distributions for Bayesian methods
  priors: null                                # Advanced prior specifications (rarely needed)

# ==============================================================================
# OPTIMIZATION METHODS
# ==============================================================================
# Optimization method selection and configuration
optimization:
  method: "nlsq"                              # Options: "nlsq" | "mcmc"
                                              # Note: NUTS/CMC is auto-selected within MCMC based on dual criteria

  # ---------------------------------------------------------------------------
  # NLSQ - Trust-Region Nonlinear Least Squares (Primary Method)
  # ---------------------------------------------------------------------------
  # Fast, deterministic optimization using Levenberg-Marquardt algorithm
  # CPU-optimized with JAX 0.8.0 (v2.3.0+), JIT-compiled for performance
  # Automatic strategy selection based on dataset size:
  #   < 1M points     → STANDARD (curve_fit)
  #   1M-10M points   → LARGE (curve_fit_large)
  #   10M-100M points → CHUNKED (curve_fit_large with progress)
  #   > 100M points   → STREAMING (unlimited data with checkpointing)
  nlsq:
    max_iterations: 100                       # Maximum optimization iterations
    tolerance: 1e-8                           # Convergence tolerance
    trust_region_scale: 1.0                   # Trust region scaling factor (0.1-10.0)
    verbose: false                            # Print iteration details

    # Parameter-Specific Scaling (Optional for Static Mode)
    # ------------------------------------------------------
    # Less critical than laminar flow, but can help if alpha has large gradients
    # Use gradient diagnostics if optimization shows signs of premature convergence:
    #   python scripts/diagnose_gradients.py --results-dir ./homodyne_results/nlsq
    x_scale_map: null                         # Per-parameter scaling (null = uniform scaling)
    # Example if gradient diagnostics shows imbalance:
    # x_scale_map:
    #   D0: 1.0
    #   alpha: 0.01                            # If alpha gradient is 100× larger
    #   D_offset: 1.0

  # ---------------------------------------------------------------------------
  # STRATIFICATION - Angle-Stratified Chunking (v2.2+)
  # ---------------------------------------------------------------------------
  # Fixes per-angle parameter compatibility with NLSQ chunking for large datasets
  # Automatically reorganizes data to ensure all chunks contain all phi angles
  # Prevents silent optimization failures (0 iterations, zero gradients)
  stratification:
    enabled: "auto"                           # Options: "auto" | true | false
                                              # "auto" activates when: per_angle_scaling=True AND n_points>=100k
    target_chunk_size: 100000                 # Target size for stratified chunks [points]
                                              # Should match NLSQ's internal chunk size
    max_imbalance_ratio: 5.0                  # Maximum angle imbalance ratio before fallback
                                              # Fallback to sequential if max_count/min_count > threshold
    force_sequential_fallback: false          # Force sequential per-angle optimization
                                              # Useful for highly imbalanced angle distributions
    check_memory_safety: true                 # Check available memory before stratification
                                              # Warns if peak memory > 70% of available
    use_index_based: false                    # Use zero-copy index-based stratification
                                              # Reduces memory overhead from 2x to ~1%
                                              # Recommended for very large datasets (>10M points)
    collect_diagnostics: false                # Collect detailed stratification diagnostics
                                              # Performance metrics, chunk balance, angle coverage
                                              # Minimal overhead (~0.01s)
    log_diagnostics: false                    # Log diagnostic report to console
                                              # Requires collect_diagnostics=true
                                              # Useful for troubleshooting and validation

  # Performance Impact:
  # - Time overhead: <1% (0.15s for 3M points)
  # - Memory overhead: 2x peak (temporary) or ~1% (index-based)
  # - Diagnostics overhead: <0.1% (0.01s for 3M points)
  # - Fixes: Silent failures with per-angle scaling on large datasets

  # ---------------------------------------------------------------------------
  # SEQUENTIAL - Sequential Per-Angle Optimization (v2.2+)
  # ---------------------------------------------------------------------------
  # Fallback strategy when stratification cannot be applied
  # Optimizes each phi angle independently and combines results
  # Used when: angle imbalance > threshold OR force_sequential_fallback=true
  sequential:
    min_success_rate: 0.5                     # Minimum fraction of angles that must converge
                                              # Optimization fails if fewer angles converge
    weighting: "inverse_variance"             # Result combination method
                                              # Options: "inverse_variance" | "uniform" | "n_points"
                                              # "inverse_variance" provides optimal statistical weighting

  # Usage Notes:
  # - Automatically activated when: imbalance_ratio > max_imbalance_ratio
  # - Can be forced via: optimization.stratification.force_sequential_fallback=true
  # - Uses scipy.optimize.least_squares per angle (CPU only)
  # - Combines results using weighted averaging

  # ---------------------------------------------------------------------------
  # MCMC - Markov Chain Monte Carlo (Uncertainty Quantification)
  # ---------------------------------------------------------------------------
  # Provides full posterior distributions and uncertainty estimates
  # Automatic NUTS/CMC selection based on dual criteria (v2.1.0):
  #   - (num_samples >= 15) OR (memory > 30%) → CMC (parallel sampling)
  #   - Otherwise → NUTS (sequential sampling)
  # Uses NumPyro or BlackJAX backend
  # No automatic NLSQ/SVI initialization (v2.1.0+) - manual workflow required
  # For static mode with 3 parameters: faster convergence than laminar_flow
  mcmc:
    num_warmup: 1000                          # NUTS warmup/adaptation samples
    num_samples: 2000                         # Posterior samples per chain
    num_chains: 4                             # Number of parallel chains (recommended: 4)
    progress_bar: true                        # Show sampling progress
    target_accept_prob: 0.8                   # NUTS target acceptance probability (0.6-0.9)
    max_tree_depth: 10                        # Maximum NUTS tree depth
    backend: "numpyro"                        # Options: "numpyro" | "blackjax"

    # Automatic NUTS/CMC selection thresholds (v2.1.0)
    # CMC selected when: (num_samples >= min_samples_for_cmc) OR (memory > memory_threshold_pct)
    min_samples_for_cmc: 15                   # Parallelism threshold (samples/angles >= 15)
    memory_threshold_pct: 0.30                # Memory threshold (30% of available)
    dense_mass_matrix: false                  # False: diagonal (faster), True: full covariance

    # Chain starting values (NOT automatic NLSQ/SVI initialization - removed in v2.1.0)
    init_strategy: "median"                   # Options: "median" | "uniform" | "prior"
                                              # Uses physics-informed priors from ParameterSpace

    # Diagnostics and quality control
    check_hmc_diagnostics: true               # Verify convergence diagnostics
    min_ess: 400                              # Minimum effective sample size per parameter
    max_rhat: 1.1                             # Maximum R-hat for convergence

  # ---------------------------------------------------------------------------
  # STREAMING - For Large Datasets (> 100M points)
  # ---------------------------------------------------------------------------
  # Constant memory footprint with checkpoint/resume capability
  streaming:
    enable_checkpoints: true                  # Enable HDF5 checkpoint save/resume
    checkpoint_dir: "./checkpoints"           # Directory for checkpoint files
    checkpoint_frequency: 10                  # Save checkpoint every N batches
    resume_from_checkpoint: true              # Auto-detect and resume from latest checkpoint
    keep_last_checkpoints: 3                  # Number of recent checkpoints to keep (older deleted)

    # Fault tolerance and error recovery
    enable_fault_tolerance: true              # Enable numerical validation and recovery
    max_retries_per_batch: 2                  # Maximum retry attempts per failed batch
    min_success_rate: 0.5                     # Minimum batch success rate (0.0-1.0) before failing

    # Batch processing
    batch_size: null                          # Batch size in points (null = auto-detect based on memory)
    adaptive_batching: true                   # Dynamically adjust batch size based on performance

  # ---------------------------------------------------------------------------
  # CMC - Consensus Monte Carlo (Automatically Selected by MCMC)
  # ---------------------------------------------------------------------------
  # CMC is automatically selected when MCMC dual criteria met (v2.1.0):
  #   (num_samples >= 15) OR (memory > 30%)
  # This section configures CMC behavior when auto-selected
  # Parallelizes MCMC across data shards and combines subposteriors
  cmc:
    enable: "auto"                            # Options: false | true | "auto" (recommended: "auto")
    min_points_for_cmc: 1000000               # Minimum dataset size to trigger CMC (legacy parameter)
    backend: "jax"                            # Options: "jax" (CPU-only in v2.3.0+) | "numpy"
    diagonal_correction: true                 # Apply diagonal correction to correlation matrix

    # Data sharding configuration
    sharding:
      strategy: "stratified"                  # Options: "stratified" | "random" | "contiguous"
      num_shards: "auto"                      # Number of shards (int or "auto" for automatic)
      max_points_per_shard: "auto"            # Maximum points per shard (int or "auto")

    # Backend configuration for parallel execution
    backend_config:
      name: "auto"                            # Options: "auto" | "pjit" | "multiprocessing" | "pbs" | "slurm"
      enable_checkpoints: true                # Enable checkpoint functionality
      checkpoint_frequency: 1                 # Save checkpoint every N shards
      checkpoint_dir: "./cmc_checkpoints"     # Directory for CMC checkpoint files
      keep_last_checkpoints: 3                # Number of recent checkpoints to keep
      resume_from_checkpoint: true            # Auto-resume from latest checkpoint

    # Subposterior combination method
    combination:
      method: "weighted_gaussian"             # Options: "weighted_gaussian" | "simple_average" | "auto"
      validate_results: true                  # Validate combined posterior quality
      min_success_rate: 0.8                   # Minimum fraction of shards that must converge (0.0-1.0)

    # Per-shard MCMC configuration
    per_shard_mcmc:
      num_warmup: 500                         # Warmup steps per shard
      num_samples: 1000                       # Samples per shard
      num_chains: 2                           # Chains per shard
      subsample_size: "auto"                  # Subsample size (int or "auto" for automatic subsampling)

    # Convergence validation
    validation:
      strict_mode: false                      # Fail if validation criteria not met
      min_per_shard_ess: 100                  # Minimum effective sample size per parameter per shard
      max_per_shard_rhat: 1.2                 # Maximum R-hat per parameter per shard
      max_between_shard_kl: 0.5               # Maximum KL divergence between shard posteriors
      min_success_rate: 0.8                   # Minimum fraction of shards that must converge

# ==============================================================================
# NOISE ESTIMATION (Optional)
# ==============================================================================
# Automatic noise level estimation using hybrid NumPyro approach
# Estimates per-angle or global noise variance to improve fit quality
noise_estimation:
  enabled: false                              # Enable automatic noise estimation
  model: "per_angle"                          # Options: "per_angle" | "global"

  # Adam optimization settings for noise parameter estimation
  adam_config:
    learning_rate: 0.01                       # Adam learning rate
    max_epochs: 500                           # Maximum optimization epochs
    convergence_threshold: 1e-6               # Convergence threshold for loss
    early_stopping: true                      # Enable early stopping for efficiency

  # Posterior sampling for uncertainty quantification
  posterior_samples: 1200                     # Number of posterior samples for noise uncertainty

  # Quality control for noise estimation
  validation:
    check_convergence: true                   # Verify optimization convergence
    reasonable_range: [1e-4, 1.0]             # Expected noise range (bounds for sanity check)
    warn_outliers: true                       # Warn about unusual noise estimates

  # Per-angle model settings
  per_angle:
    min_angles_required: 2                    # Minimum angles needed for per-angle estimation
    validate_coverage: true                   # Ensure proper angle coverage

# ==============================================================================
# PERFORMANCE OPTIMIZATION
# ==============================================================================
# Performance and memory management settings
performance:
  # Strategy override (null = automatic selection based on dataset size)
  strategy_override: null                     # Options: null | "standard" | "large" | "chunked" | "streaming"

  # Memory management
  memory_limit_gb: null                       # Custom memory limit in GB (null = auto-detect)
  enable_progress: true                       # Show progress bars during optimization

  # Memory optimization settings
  memory_optimization:
    enabled: true                             # Enable memory optimization
    max_memory_usage_gb: 6.0                  # Maximum memory usage [GB]
    chunk_size: 8000                          # Data chunk size for processing
    enable_caching: true                      # Enable intelligent caching
    cache_strategy: "adaptive"                # Options: "adaptive" | "aggressive" | "conservative"

  # Computation settings
  computation:
    enable_jit: true                          # Enable JAX JIT compilation
    cpu_threads: "auto"                       # Number of CPU threads (int or "auto")
    vectorization_level: "high"               # Options: "low" | "medium" | "high"
    # Note: GPU support removed in v2.3.0 (use v2.2.x for GPU)

# ==============================================================================
# LOGGING
# ==============================================================================
# Logging configuration for analysis monitoring
logging:
  enabled: true                               # Enable logging
  level: "INFO"                               # Options: "DEBUG" | "INFO" | "WARNING" | "ERROR"

  # Console logging
  console:
    enabled: true                             # Log to console
    level: "INFO"                             # Console log level
    format: "detailed"                        # Options: "simple" | "detailed"
    colors: true                              # Enable colored output
    show_progress: true                       # Show progress indicators

  # File logging
  file:
    enabled: false                            # Log to file
    level: "DEBUG"                            # File log level
    path: "./logs/"                           # Log directory
    filename: "homodyne_static_analysis.log"  # Log file name
    max_size_mb: 10                           # Maximum log file size before rotation
    backup_count: 5                           # Number of backup log files to keep

  # Module-specific logging levels
  modules:
    "homodyne.data.phi_filtering": "INFO"
    "homodyne.data.xpcs_loader": "INFO"
    "homodyne.optimization.nlsq_wrapper": "INFO"
    "jax._src": "WARNING"                     # Suppress JAX internal messages

# ==============================================================================
# QUALITY CONTROL (Optional)
# ==============================================================================
# Advanced quality control for data validation and analysis monitoring
quality_control:
  enabled: false                              # Enable quality control checks

  # Angle-specific quality checks
  angle_quality:
    validate_angle_coverage: true             # Validate angle coverage adequacy
    min_angles_per_range: 1                   # Minimum angles required per target range
    check_angle_distribution: true            # Check angle distribution uniformity

  # Multi-angle data validation
  multi_angle_validation:
    check_correlation_consistency: true       # Check correlation consistency across angles
    validate_angle_dependencies: true         # Validate physical angle dependencies
    detect_anomalous_angles: true             # Detect and warn about anomalous angles

# ==============================================================================
# VISUALIZATION (Optional)
# ==============================================================================
# Plot generation and output configuration
plotting:
  save_plots: true                            # Save plots to output directory
  show_plots: false                           # Display plots interactively
  format: "png"                               # Options: "png" | "pdf" | "svg"
  dpi: 300                                    # Resolution [dots per inch]
  style: "publication"                        # Plot style (matplotlib style name)

  # Rendering mode selection (hybrid plotting system)
  # Controls speed vs quality tradeoff for C2 heatmap plots
  preview_mode: false                         # false = publication quality (matplotlib, slower)
                                              # true = fast preview (Datashader, 5-10x faster)
  fit_surface: "solver"                       # Options: "solver" | "posthoc"

  color_scale:
    mode: "legacy"
    pin_legacy_range: true
    percentile_min: 1.0
    percentile_max: 99.0
    fixed_min: 1.0
    fixed_max: 1.5

  # Datashader configuration (used when preview_mode: true)
  datashader:
    canvas_width: 1200                        # Rendering resolution in pixels
    canvas_height: 1200                       # Higher values = more detail, larger files
    # Note: CPU-only rendering in v2.3.0+

  # Matplotlib configuration (used when preview_mode: false)
  matplotlib:
    interpolation: "bilinear"                 # Options: "none" | "bilinear" | "bicubic"
    use_tight_layout: true                    # Use tight layout for plots
    savefig_kwargs:
      bbox_inches: "tight"
      pad_inches: 0.1

  # Plot types to generate
  correlation_function: true                  # C1(t) and C2(t1,t2) plots
  fit_quality: true                           # Residuals and fit quality plots
  parameter_distributions: true               # Parameter posterior distributions (MCMC only)
  residual_analysis: true                     # Residual analysis plots

  # Angle-specific plots
  angle_coverage: true                        # Show which angles were used
  angle_correlation: true                     # Correlation quality vs angle

# ==============================================================================
# OUTPUT
# ==============================================================================
# Output file configuration
output:
  directory: "./results"                      # Base output directory
  base_directory: "./homodyne_results/"       # Alternative: more descriptive directory name

  # Output formats
  formats:
    hdf5: true                                # Save results in HDF5 format
    json: true                                # Save results in JSON format
    csv: true                                 # Save results in CSV format (tables only)

  # Output organization
  create_subdirs: true                        # Create method-specific subdirectories (nlsq/, mcmc/, cmc/)
  timestamp_dirs: false                       # Add timestamp to directory names

  # Compression
  compress_hdf5: true                         # Enable HDF5 compression (saves disk space)
  compression_level: 6                        # HDF5 compression level (0-9, higher = more compression)

# ==============================================================================
# VALIDATION (Optional)
# ==============================================================================
# Configuration validation settings
validation:
  strict_mode: false                          # Fail on validation warnings
  check_file_existence: true                  # Verify data files exist before analysis
  validate_parameter_ranges: true             # Check parameters are within reasonable physics bounds
  check_mode_compatibility: true              # Verify configuration matches selected analysis mode

  # Angle validation
  angle_validation:
    require_multiple_angles: false            # Not required for static (can analyze single angle)
    min_angle_count: 1                        # Minimum number of angles required
    validate_angle_ranges: true               # Validate angle ranges are physically meaningful

# ==============================================================================
# STATIC DIFFUSION - USAGE NOTES
# ==============================================================================
#
# QUICK START (v2.1.0+ Manual Workflow):
# ----------------------------------------
# Step 1 - NLSQ optimization:
#   1. Update experimental_data paths to your HDF5 file
#   2. Adjust phi_filtering target_ranges based on your system
#   3. Run: homodyne --config this_file.yaml --method nlsq
#   4. Note best-fit parameters from output
#
# Step 2 - Manual MCMC initialization (no automatic initialization in v2.1.0+):
#   5. Manually copy NLSQ best-fit results from output
#   6. Update initial_parameters.values in this file with NLSQ results
#   7. Example: values: [1234.5, 0.567, 12.34]  # 3 parameters for static
#   8. Run: homodyne --config this_file.yaml --method mcmc
#   9. Automatic NUTS/CMC selection based on dual criteria
#
# STATIC DIFFUSION PHYSICS:
# -------------------------
# Model: D(t,φ) = D₀·t^α + D_offset
#
# Parameters:
#   D₀: Diffusion coefficient prefactor (magnitude)
#   α: Anomalous diffusion exponent (0=normal, <0=sub, >0=super)
#   D_offset: Baseline/constant diffusion contribution
#
# Typical values:
#   D₀: 100-10000 Å²/s for colloidal systems
#   α: -2 to 2 for equilibrium systems
#   D_offset: Usually small (-100 to 100 Å²/s)
#
# PARAMETER COUNTING:
# -------------------
# Total = 3 physical + 2 × N_angles scaling
# Example with 3 filtered angles: 3 + 2×3 = 9 parameters
#   Physical: [D₀, α, D_offset]
#   Scaling: [contrast₁, offset₁, contrast₂, offset₂, contrast₃, offset₃]
#
# ANGLE FILTERING FOR STATIC:
# ----------------------------
# Recommended ranges:
#   - 0° (parallel): Detect primary anisotropy axis
#   - 180° (antiparallel): Verify symmetric behavior
#   - Optional 90° (perpendicular): Full anisotropic characterization
#
# Use fallback_to_all_angles: true if anisotropy direction unknown
#
# OPTIMIZATION METHOD SELECTION (v2.1.0):
# ----------------------------------------
# NLSQ (Recommended first):
#   - Fast: ~seconds to minutes
#   - CPU-optimized with JAX 0.8.0 JIT compilation
#   - Deterministic point estimates
#   - Good for exploration and initial fit
#
# MCMC (For publication with uncertainty quantification):
#   - Slower: ~minutes to hours (faster than laminar_flow for 3 params)
#   - Full posterior distributions with credible intervals
#   - Automatic NUTS/CMC selection based on dual criteria:
#     * (num_samples >= 15) OR (memory > 30%) → CMC (parallel sampling)
#     * Otherwise → NUTS (sequential sampling)
#   - Convergence diagnostics (R-hat, ESS, divergences)
#   - Manual workflow (v2.1.0+): Run NLSQ → copy results → update YAML → run MCMC
#   - No automatic NLSQ/SVI initialization (removed in v2.1.0)
#
# Streaming (Automatic for >100M points):
#   - Constant memory footprint
#   - Checkpoint/resume capability
#
# DATASET SIZE HANDLING:
# ----------------------
# NLSQ automatic strategy selection:
#   < 1M points     → STANDARD
#   1M-10M points   → LARGE
#   10M-100M points → CHUNKED
#   > 100M points   → STREAMING
#
# PLATFORM SUPPORT (v2.3.0):
# ---------------------------
# CPU-only: Linux, macOS, Windows (full support, multi-core optimized)
# GPU support removed in v2.3.0 (use v2.2.1 for GPU features)
# HPC-ready: Optimized for 14+ core CPUs, tested on 36-128 core nodes
#
# Installation:
#   pip install homodyne  # Automatically installs CPU-only JAX 0.8.0
#
# Requirements:
#   - Python 3.12+
#   - JAX==0.8.0 and jaxlib==0.8.0 (exact match required, CPU-only)
#   - For GPU: Stay on homodyne v2.2.1 (last GPU-supporting version)
#
# VALIDATION:
# -----------
# homodyne-config --validate this_file.yaml
# python -m homodyne.runtime.utils.system_validator --quick
#
# ==============================================================================
# END OF STATIC DIFFUSION TEMPLATE
# ==============================================================================
