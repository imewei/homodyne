{
  "metadata": {
    "_comment": "Large Dataset Static Isotropic Configuration - Optimized for 1-20M data points (centered on 5M). Aggressive performance settings with relaxed tolerances for high-speed processing.",
    "config_version": "0.7.2",
    "description": "Large dataset static isotropic homodyne scattering analysis - high-performance 3-parameter model optimized for datasets with 1-20M data points",
    "based_on": "He et al. PNAS 2024 - Transport coefficient approach (static case) with large dataset optimizations",
    "analysis_mode": "static_isotropic",
    "dataset_optimization": "large_dataset_optimized",
    "plotting_features": {
      "experimental_data_plotting": "Use --plot-experimental-data or 'hexp' shortcut for validation",
      "simulated_data_plotting": "Use --plot-simulated-data or 'hsim' shortcut with --contrast and --offset parameters",
      "scaling_examples": [
        "--contrast 0.15 --offset 1.0",
        "--phi-angles 0,45,90,135"
      ],
      "multi_angle_support": "Individual plots for each phi angle with optimized color scaling (isotropic mode uses minimal angles)"
    },
    "parameters_optimized": [
      "D0",
      "alpha",
      "D_offset"
    ],
    "angle_filtering": "disabled_for_isotropic",
    "shell_completion": "Use argcomplete for shell completion (conda environments auto-enabled)",
    "script_compatibility": "homodyne v0.7.2",
    "large_dataset_notes": "Aggressive performance settings with relaxed tolerances (5e-6) and optimized sampling for efficient processing of large datasets"
  },
  "experimental_data": {
    "_comment": "Data source and loading configuration optimized for large datasets",
    "data_folder_path": "./data/SAMPLE_NAME/",
    "data_file_name": "your_data_file.hdf",
    "phi_angles_path": "./data/phi_angles/",
    "phi_angles_file": "phi_list.txt",
    "exchange_key": "exchange",
    "cache_file_path": "./data/SAMPLE_NAME/",
    "cache_filename_template": "cached_c2_frames_{start_frame}_{end_frame}.npz",
    "cache_compression": true,
    "_cache_compression_note": "Enabled for large dataset storage optimization",
    "data_type": "float32",
    "_data_type_note": "Memory-efficient float32 for large datasets - minimal precision loss",
    "file_format": "NPZ",
    "preprocessing": {
      "apply_diagonal_correction": true,
      "vectorized_correction": true,
      "cache_processed_data": true,
      "normalize_data": true,
      "normalization_method": "baseline"
    }
  },
  "analyzer_parameters": {
    "_comment": "Core analysis parameters - static isotropic mode optimized for large datasets",
    "temporal": {
      "dt": 0.1,
      "_dt_note": "Time step between frames in seconds",
      "dt_unit": "seconds",
      "start_frame": 100,
      "_start_frame_note": "Large dataset optimized: standard start for skip initial equilibration",
      "end_frame": 5000100,
      "_end_frame_note": "Large dataset optimized: ~5M frame analysis window for comprehensive large dataset analysis",
      "frame_description": "Analysis window for large dataset experiment (1-20M frames, centered on 5M)"
    },
    "scattering": {
      "wavevector_q": 0.001,
      "_q_note": "Scattering wavevector magnitude: q = 4\u03c0 sin(\u03b8/2)/\u03bb",
      "q_unit": "\u00c5\u207b\u00b9",
      "typical_range": [
        0.001,
        0.1
      ]
    },
    "geometry": {
      "stator_rotor_gap": 2000000,
      "_gap_note": "Gap in Angstroms (not critical for static isotropic mode)",
      "gap_unit": "\u00c5",
      "gap_in_microns": 200
    },
    "computational": {
      "num_threads": "auto",
      "auto_detect_cores": true,
      "max_threads_limit": 32,
      "_max_threads_note": "Large dataset optimization: aggressive threading for high-performance processing",
      "memory_limit_gb": 128,
      "_memory_limit_note": "Large dataset optimization: high memory allocation for efficient processing"
    }
  },
  "initial_parameters": {
    "_comment": "Starting values for 3-parameter static isotropic optimization [D0, alpha, D_offset]",
    "values": [
      100.0,
      0.0,
      10.0
    ],
    "parameter_names": [
      "D0",
      "alpha",
      "D_offset"
    ],
    "units": [
      "\u00c5\u00b2/s",
      "dimensionless",
      "\u00c5\u00b2/s"
    ],
    "physical_meaning": {
      "D0": "Reference diffusion coefficient",
      "alpha": "Power-law exponent for D(t) evolution",
      "D_offset": "Baseline diffusion coefficient"
    },
    "active_parameters": [
      "D0",
      "alpha",
      "D_offset"
    ],
    "parameter_units": {
      "D0": "\u00c5\u00b2/s",
      "alpha": "dimensionless",
      "D_offset": "\u00c5\u00b2/s"
    }
  },
  "optimization_config": {
    "_comment": "Optimization configuration with angle filtering disabled for isotropic mode and aggressive large dataset performance",
    "angle_filtering": {
      "enabled": false,
      "_isotropic_note": "Angle filtering disabled for isotropic analysis - no angular dependence",
      "target_ranges": [],
      "fallback_to_all_angles": true,
      "_fallback_note": "Use all angles if no angles found in target ranges",
      "_performance_note": "Filtering disabled - isotropic symmetry assumes no angle dependence"
    },
    "classical_optimization": {
      "methods": [
        "Nelder-Mead",
        "Gurobi"
      ],
      "_methods_note": "Classical methods: Nelder-Mead (always available), Gurobi (if licensed). Robust methods are in separate robust_optimization section.",
      "_usage_flags": {
        "--method classical": "Runs ONLY traditional classical methods: Nelder-Mead, Gurobi (if available)",
        "--method robust": "Runs ONLY robust methods: Robust-Wasserstein, Robust-Scenario, Robust-Ellipsoidal",
        "--method mcmc": "Runs Bayesian MCMC sampling for uncertainty quantification",
        "--method all": "Runs classical + robust + MCMC methods for comprehensive analysis"
      },
      "_gurobi_availability": "Gurobi quadratic programming solver provides an alternative to Nelder-Mead using quadratic approximation of the chi-squared objective function. Requires Gurobi license and installation (pip install gurobipy).",
      "method_options": {
        "Nelder-Mead": {
          "maxiter": 1500,
          "_maxiter_note": "Large dataset optimization: reduced from default for faster convergence with high SNR",
          "xatol": 5e-06,
          "_xatol_note": "Large dataset optimization: relaxed parameter tolerance leveraging statistical power",
          "fatol": 5e-06,
          "_fatol_note": "Large dataset optimization: relaxed function tolerance for efficiency",
          "adaptive": true,
          "_adaptive_note": "Adaptive step sizes essential for efficient convergence"
        },
        "Gurobi": {
          "_comment": "ENHANCED: Now uses iterative Trust Region SQP instead of single-shot QP (fixes \u03c7\u00b2 convergence)",
          "_algorithm": "Iterative trust region SQP optimization with quadratic approximation and adaptive radius",
          "max_iterations": 200,
          "_max_iterations_note": "Large dataset optimization: reduced iterations leveraging statistical power",
          "tolerance": 5e-06,
          "_tolerance_note": "Large dataset optimization: relaxed convergence tolerance for efficiency",
          "output_flag": 0,
          "_output_flag_note": "Gurobi output verbosity (0=silent, 1=normal)",
          "method": 2,
          "_method_note": "Gurobi solution method (2=barrier method recommended for QP)",
          "time_limit": 120,
          "_time_limit_note": "Large dataset optimization: aggressive time limit for high-throughput processing",
          "_trust_region_settings": {
            "initial_radius": 10.0,
            "_initial_radius_note": "Large dataset optimization: larger radius for aggressive exploration",
            "max_radius": 100.0,
            "_max_radius_note": "Large dataset optimization: large max radius for efficiency",
            "eta1": 0.15,
            "eta2": 0.75,
            "gamma1": 0.8,
            "_gamma1_note": "Large dataset optimization: aggressive shrinking factor",
            "gamma2": 4.0,
            "_gamma2_note": "Large dataset optimization: aggressive expansion for fast convergence",
            "max_trust_iterations": 100,
            "_note": "Large dataset optimization: aggressive trust region parameters for performance"
          },
          "_gurobi_isotropic_note": "Gurobi particularly effective for isotropic mode with large datasets - no angle filtering complexity",
          "_usage_note": "Gurobi uses quadratic approximation via finite differences. Best for smooth objective functions with bounds constraints.",
          "_advantages": "Handles bounds constraints naturally, can be faster for well-conditioned problems",
          "_disadvantages": "Requires license, uses quadratic approximation which may not capture all nonlinearity"
        }
      },
      "selection_strategy": "best_chi_squared",
      "_selection_strategy_note": "How to choose best result when multiple methods are used: best_chi_squared, consensus, first_success",
      "_optimization_note": "3-parameter isotropic optimization with large datasets - efficient with good statistics",
      "objective_function": {
        "type": "standard",
        "_type_note": "Objective function type: \"standard\" for min \u03c7\u00b2 or \"adaptive_target\" for min (\u03c7\u00b2 - \u03b1\u00b7DOF)\u00b2",
        "_type_options": {
          "standard": "Traditional chi-squared minimization (default)",
          "adaptive_target": "Adaptive target chi-squared to prevent overfitting"
        },
        "adaptive_target_alpha": 1.0,
        "_adaptive_target_alpha_note": "Target multiplier \u03b1 for adaptive chi-squared (recommended range: 0.8-1.2)",
        "_adaptive_target_description": "When type=\"adaptive_target\", minimizes (\u03c7\u00b2 - \u03b1\u00b7DOF)\u00b2 where DOF = N_data - N_params",
        "_adaptive_target_benefits": "Prevents overfitting by targeting statistically reasonable chi-squared values"
      }
    },
    "robust_optimization": {
      "_comment": "Robust optimization settings using CVXPY with optimized performance (CLARABEL/SCS solvers) for large datasets",
      "_usage_examples": [
        "python run_homodyne.py --method robust  # Run only robust methods",
        "python run_homodyne.py --method robust --static-isotropic  # Robust in isotropic mode",
        "python run_homodyne.py --method classical  # Run all methods including robust",
        "python run_homodyne.py --method all  # Run classical (with robust) + MCMC"
      ],
      "_robust_flag_benefits": "Use --method robust for noise-resistant estimation without classical methods - efficient for large datasets",
      "_performance_note": "v0.7.2+ optimizations: 100x-5000x speedup with caching, adaptive Jacobians, and streamlined solvers",
      "enabled": true,
      "uncertainty_model": "wasserstein",
      "_uncertainty_model_note": "Options: wasserstein, ellipsoidal, scenario",
      "uncertainty_radius": 0.02,
      "_uncertainty_radius_note": "Large dataset optimization: reduced uncertainty radius leveraging statistical power",
      "n_scenarios": 10,
      "_n_scenarios_note": "Large dataset optimization: reduced scenarios for efficiency - large datasets provide robustness",
      "regularization_alpha": 0.005,
      "_regularization_alpha_note": "Large dataset optimization: reduced L2 regularization leveraging robustness",
      "regularization_beta": 0.0005,
      "_regularization_beta_note": "Large dataset optimization: reduced L1 sparsity parameter for efficiency",
      "jacobian_epsilon": 1e-05,
      "_jacobian_epsilon_note": "Large dataset optimization: relaxed finite difference step for performance",
      "enable_caching": true,
      "_enable_caching_note": "Enable performance caching for repeated computations - major gains for large datasets",
      "preferred_solver": "CLARABEL",
      "_preferred_solver_note": "Preferred CVXPY solver: CLARABEL, SCS, CVXOPT",
      "solver_settings": {
        "_comment": "Performance-optimized solver configuration for CVXPY-based robust optimization with large datasets",
        "_solver_hierarchy": "CLARABEL (default) -> SCS (fallback) -> CVXOPT (last resort)",
        "CLARABEL": {
          "_description": "Interior-point solver optimized for 3-parameter static isotropic fitting with large datasets",
          "max_iter": 300,
          "_max_iter_note": "Large dataset optimization: reduced iterations for performance",
          "tol_gap_abs": 0.0001,
          "tol_gap_rel": 0.0001,
          "tol_feas": 1e-05,
          "tol_infeas_abs": 0.0001,
          "tol_infeas_rel": 0.0001,
          "tol_ktratio": 0.001,
          "equilibrate_enable": true,
          "equilibrate_max_iter": 20,
          "_equilibrate_max_iter_note": "Large dataset optimization: reduced equilibration for speed",
          "equilibrate_min_scaling": 1e-06,
          "equilibrate_max_scaling": 1000000.0,
          "direct_kkt_solver": true,
          "static_regularization_enable": true,
          "static_regularization_constant": 1e-06,
          "dynamic_regularization_enable": true,
          "iterative_refinement_enable": false,
          "_iterative_refinement_note": "Large dataset optimization: disabled for performance",
          "presolve_enable": true,
          "verbose": false
        },
        "SCS": {
          "_description": "Conic solver optimized for 3-parameter large dataset problems",
          "max_iters": 5000,
          "_max_iters_note": "Large dataset optimization: reduced iterations for speed",
          "eps": 0.001,
          "alpha": 1.8,
          "rho_x": 1e-05,
          "scale": 10.0,
          "normalize": true,
          "adaptive_scale": true,
          "acceleration_lookback": 10,
          "_acceleration_lookback_note": "Large dataset optimization: reduced lookback for speed",
          "acceleration_interval": 5,
          "_acceleration_interval_note": "Large dataset optimization: frequent acceleration updates",
          "time_limit_secs": 180,
          "_time_limit_secs_note": "Large dataset optimization: aggressive time limit",
          "verbose": false
        },
        "CVXOPT": {
          "_description": "Stable fallback solver for 3-parameter large dataset problems",
          "maxiters": 100,
          "_maxiters_note": "Large dataset optimization: reduced iterations for speed",
          "abstol": 0.0001,
          "reltol": 0.001,
          "feastol": 0.0001,
          "refinement": 1,
          "_refinement_note": "Large dataset optimization: minimal refinement for speed",
          "verbose": false
        },
        "GUROBI": {
          "_description": "Commercial solver optimized for 3-parameter large dataset optimization",
          "_availability": "Requires Gurobi license",
          "Method": 2,
          "_Method_note": "Barrier method with enhanced settings",
          "CrossOver": -1,
          "_CrossOver_note": "Auto-decide crossover for stability",
          "BarHomogeneous": 1,
          "_BarHomogeneous_note": "Homogeneous barrier for complex problems",
          "BarIterLimit": 500,
          "TimeLimit": 180,
          "_TimeLimit_note": "Large dataset optimization: standard timeout for efficiency",
          "MIPGap": 0.01,
          "NumericFocus": 1,
          "_NumericFocus_note": "Balanced numerical focus for performance",
          "OutputFlag": 0,
          "_OutputFlag_note": "Suppress solver output",
          "ScaleFlag": 3,
          "BarConvTol": 1e-05,
          "FeasibilityTol": 0.0001,
          "OptimalityTol": 0.0001,
          "MarkowitzTol": 0.2,
          "PerturbValue": 0.0005,
          "Presolve": 2
        }
      },
      "method_options": {
        "wasserstein": {
          "uncertainty_radius": 0.02,
          "_uncertainty_radius_note": "Large dataset optimization: reduced radius",
          "regularization_alpha": 0.005,
          "_regularization_alpha_note": "Large dataset optimization: reduced regularization"
        },
        "scenario": {
          "n_scenarios": 10,
          "_n_scenarios_note": "Large dataset optimization: reduced scenarios for efficiency",
          "bootstrap_method": "residual"
        },
        "ellipsoidal": {
          "gamma": 0.05,
          "_gamma_note": "Large dataset optimization: reduced ellipsoidal parameter",
          "regularization_alpha": 0.005,
          "_regularization_alpha_note": "Large dataset optimization: reduced regularization"
        }
      },
      "objective_function": {
        "type": "standard",
        "_type_note": "Objective function type: \"standard\" for min \u03c7\u00b2 or \"adaptive_target\" for min (\u03c7\u00b2 - \u03b1\u00b7DOF)\u00b2",
        "_type_options": {
          "standard": "Traditional chi-squared minimization (default)",
          "adaptive_target": "Adaptive target chi-squared to prevent overfitting"
        },
        "adaptive_target_alpha": 1.0,
        "_adaptive_target_alpha_note": "Target multiplier \u03b1 for adaptive chi-squared (recommended range: 0.8-1.2)",
        "_adaptive_target_description": "When type=\"adaptive_target\", minimizes (\u03c7\u00b2 - \u03b1\u00b7DOF)\u00b2 where DOF = N_data - N_params",
        "_adaptive_target_benefits": "Prevents overfitting by targeting statistically reasonable chi-squared values in robust optimization"
      }
    },
    "mcmc_sampling": {
      "_comment": "Isolated MCMC Backend Architecture - Large Dataset Static Isotropic Mode (3 parameters: D0, alpha, D_offset) - Efficient sampling with large dataset optimizations",
      "_backend_architecture": {
        "isolation": "Complete separation of PyMC CPU and NumPyro GPU implementations",
        "cpu_implementation": "homodyne.optimization.mcmc_cpu_backend.py",
        "gpu_implementation": "homodyne.optimization.mcmc_gpu_backend.py",
        "conflict_prevention": "Eliminates PyTensor/JAX namespace conflicts through isolated execution",
        "backend_selection": "HOMODYNE_GPU_INTENT environment variable or command-based selection",
        "complexity_note": "3-parameter isotropic model with large datasets benefits from efficient sampling and GPU acceleration"
      },
      "enabled": true,
      "sampler": "NUTS",
      "draws": 2000,
      "_draws_note": "Large dataset optimization: reduced draws for efficiency - large datasets provide superior statistics",
      "tune": 600,
      "_tune_note": "Large dataset optimization: reduced tuning for speed - good initialization from data",
      "thin": 2,
      "_thin_note": "Large dataset optimization: increased thinning for memory efficiency",
      "chains": 4,
      "cores": 4,
      "target_accept": 0.8,
      "_target_accept_note": "Large dataset optimization: reduced acceptance rate for speed - statistical robustness from large N",
      "max_treedepth": 6,
      "_max_treedepth_note": "Large dataset optimization: reduced depth for efficiency - simple 3-parameter space",
      "return_inferencedata": true,
      "backend_specific": {
        "cpu_backend": {
          "_implementation": "homodyne.optimization.mcmc_cpu_backend.py",
          "_description": "Isolated PyMC implementation - optimized for 3-parameter isotropic model with large datasets",
          "_command": "homodyne --method mcmc (or HOMODYNE_GPU_INTENT=false)",
          "_isolation": "Pure PyMC environment, complete separation from NumPyro/JAX",
          "_performance_note": "CPU backend handles 3-parameter spaces efficiently with large datasets",
          "init_strategy": "adapt_diag",
          "compute_convergence_checks": true,
          "progressbar": true,
          "pytensor_config": "Automatically configured for CPU-only operation"
        },
        "gpu_backend": {
          "_implementation": "homodyne.optimization.mcmc_gpu_backend.py",
          "_description": "Isolated NumPyro/JAX implementation - GPU-accelerated 3-parameter sampling for large datasets",
          "_command": "homodyne-gpu --method mcmc (or HOMODYNE_GPU_INTENT=true)",
          "_platform_support": "Linux with CUDA preferred for optimal performance with large datasets",
          "_isolation": "Pure NumPyro/JAX environment, complete separation from PyMC/PyTensor",
          "_performance_note": "GPU acceleration provides significant speedup for large dataset processing",
          "init_strategy": "init_to_value",
          "chain_method": "vectorized",
          "progress_bar": true,
          "jit_compile": true,
          "device_memory_fraction": 0.9,
          "_device_memory_fraction_note": "Large dataset optimization: aggressive GPU memory usage",
          "jax_config": "Automatically optimized for large dataset parameter sampling"
        }
      },
      "performance_features": {
        "auto_tune_performance": true,
        "use_progressive_sampling": true,
        "use_intelligent_subsampling": true,
        "_use_intelligent_subsampling_note": "Large dataset optimization: enabled for memory management and speed",
        "enable_jit_compilation": true,
        "memory_optimization": true
      }
    },
    "scaling_parameters": {
      "_comment": "Physical scaling for c2_fitted = c2_theory * contrast + offset",
      "fitted_range": {
        "min": 1.0,
        "max": 2.0
      },
      "theory_range": {
        "min": 0.0,
        "max": 1.0
      },
      "contrast": {
        "min": 0.0001,
        "_min_note": "Large dataset optimization: broader lower bound leveraging statistical power",
        "max": 0.5,
        "prior_mu": 0.05,
        "_prior_mu_note": "Large dataset optimization: adjusted for typical large dataset characteristics",
        "prior_sigma": 0.02,
        "_prior_sigma_note": "Large dataset optimization: tighter prior leveraging precision",
        "type": "TruncatedNormal"
      },
      "offset": {
        "min": 1.0,
        "_min_note": "Large dataset optimization: tighter bound for efficiency",
        "max": 1.5,
        "_max_note": "Large dataset optimization: tighter bound for efficiency",
        "prior_mu": 1.3,
        "_prior_mu_note": "Large dataset optimization: adjusted for typical characteristics",
        "prior_sigma": 0.02,
        "_prior_sigma_note": "Large dataset optimization: tight prior for efficiency",
        "type": "TruncatedNormal"
      }
    }
  },
  "parameter_space": {
    "_comment": "Parameter bounds for 3-parameter static isotropic optimization with large dataset considerations",
    "bounds": [
      {
        "name": "D0",
        "min": 1.0,
        "max": 1000000.0,
        "type": "TruncatedNormal",
        "prior_mu": 10000.0,
        "prior_sigma": 2000.0,
        "_prior_sigma_note": "Large dataset optimization: broader prior leveraging statistical power",
        "unit": "\u00c5\u00b2/s"
      },
      {
        "name": "alpha",
        "min": -2.0,
        "max": 2.0,
        "type": "Normal",
        "prior_mu": -1.5,
        "prior_sigma": 0.2,
        "_prior_sigma_note": "Large dataset optimization: broader prior leveraging convergence robustness",
        "unit": "dimensionless"
      },
      {
        "name": "D_offset",
        "min": -100,
        "max": 100,
        "type": "Normal",
        "prior_mu": 0.0,
        "prior_sigma": 15.0,
        "_prior_sigma_note": "Large dataset optimization: broader prior for parameter exploration",
        "unit": "\u00c5\u00b2/s"
      }
    ]
  },
  "analysis_settings": {
    "_comment": "Static isotropic mode configuration for large datasets",
    "static_mode": true,
    "_static_mode_note": "True for static analysis (only D0, alpha, D_offset)",
    "static_submode": "isotropic",
    "_submode_note": "Isotropic mode - no angle dependence",
    "model_description": {
      "static_isotropic": "g\u2082(t\u2081,t\u2082) ~ [exp(-q\u00b2 \u222b D(t)dt)]\u00b2 with isotropic symmetry - large dataset performance"
    }
  },
  "advanced_settings": {
    "_comment": "Advanced computational settings for static isotropic analysis with large dataset enhancements",
    "data_loading": {
      "use_diagonal_correction": true,
      "vectorized_diagonal_fix": true
    },
    "chi_squared_calculation": {
      "method": "standard",
      "_scaling_note": "Scaling optimization always enabled for proper chi-squared",
      "minimum_sigma": 1e-08,
      "_minimum_sigma_note": "Large dataset optimization: relaxed precision for efficiency",
      "moving_window_size": 11,
      "moving_window_edge_method": "reflect",
      "fast_computation": true,
      "_fast_computation_note": "Large dataset optimization: enabled for high-speed calculations",
      "uncertainty_calculation": {
        "enable_uncertainty": true,
        "report_uncertainty": true,
        "minimum_angles_for_uncertainty": 1,
        "_minimum_angles_note": "Isotropic mode: single angle sufficient"
      },
      "validity_check": {
        "check_positive_D0": true,
        "check_positive_gamma_dot_t0": false,
        "_check_positive_gamma_dot_t0_note": "Static isotropic: no shear rate parameters",
        "check_positive_time_dependent": true,
        "check_parameter_bounds": true
      },
      "_adaptive_targeting_note": "Adaptive chi-squared targeting is configured separately in optimization_config.classical_optimization.objective_function and optimization_config.robust_optimization.objective_function",
      "variance_method": "irls_mad_robust",
      "_variance_method_note": "Options: irls_mad_robust (default), irls_optimized (50x faster), mad_robust, mad_optimized (10x faster)",
      "irls_config": {
        "max_iterations": 10,
        "damping_factor": 0.7,
        "convergence_tolerance": 0.001,
        "initial_sigma_squared": 0.001,
        "_algorithm_note": "IRLS with MAD: \u03c3\u00b2\u1d62 = (1.4826 \u00d7 MAD)\u00b2, damping: \u03c3\u00b2 = \u03b1\u00b7\u03c3\u00b2_new + (1-\u03b1)\u00b7\u03c3\u00b2_prev",
        "optimized_config": {
          "use_vectorized_mad": true,
          "use_quickselect_median": true,
          "enable_jit_compilation": true,
          "parallel_window_processing": true
        }
      },
      "performance_optimization": {
        "_comment": "Performance optimization settings for chi-squared and IRLS calculations",
        "enabled": true,
        "variance_estimator": "irls_optimized",
        "_variance_estimator_note": "irls_optimized provides 50-100x speedup via JIT compilation and vectorization",
        "chi_calculator": "vectorized_jit",
        "_chi_calculator_note": "vectorized_jit provides 20-50x speedup with memory pooling",
        "median_algorithm": "quickselect",
        "_median_algorithm_note": "quickselect provides 5-10x speedup over numpy median for small arrays",
        "jit_compilation": {
          "enabled": true,
          "warmup_on_init": true,
          "parallel_processing": true,
          "fastmath": true,
          "cache_kernels": true
        },
        "memory_optimization": {
          "use_memory_pool": true,
          "pool_size_mb": 1024,
          "adaptive_caching": true,
          "cache_strategy": "lru",
          "max_cache_items": 300
        },
        "parallel_threads": {
          "enabled": true,
          "thread_count": "auto",
          "chunk_size": "adaptive",
          "backend": "threading"
        }
      }
    },
    "numerical_integration": {
      "method": "simpson",
      "relative_tolerance": 1e-06,
      "_relative_tolerance_note": "Large dataset optimization: balanced precision for efficiency",
      "absolute_tolerance": 1e-10,
      "_absolute_tolerance_note": "Large dataset optimization: balanced precision tolerances"
    },
    "optimization_controls": {
      "convergence_tolerance": 1e-06,
      "_convergence_tolerance_note": "Large dataset optimization: relaxed from 1e-8 for performance",
      "max_function_evaluations": 3000,
      "_max_function_evaluations_note": "Large dataset optimization: reduced limits leveraging convergence efficiency",
      "parameter_scaling": "auto",
      "finite_difference_step": 1e-06,
      "_finite_difference_step_note": "Large dataset optimization: relaxed precision for performance"
    }
  },
  "performance_settings": {
    "_comment": "Performance optimization settings aggressively tuned for large dataset analysis",
    "caching": {
      "enable_memory_cache": true,
      "enable_disk_cache": true,
      "cache_size_limit_mb": 2000,
      "_cache_size_limit_mb_note": "Large dataset optimization: aggressive caching for performance",
      "auto_cleanup": true,
      "_auto_cleanup_note": "Large dataset optimization: enabled for memory management"
    },
    "parallel_processing": {
      "enable_multiprocessing": true,
      "chunk_size": "auto",
      "backend": "threading"
    },
    "memory_management": {
      "low_memory_mode": true,
      "_low_memory_mode_note": "Large dataset optimization: enabled for memory efficiency",
      "garbage_collection_frequency": 1,
      "_garbage_collection_frequency_note": "Large dataset optimization: aggressive GC for memory management",
      "memory_monitoring": true,
      "_memory_monitoring_note": "Large dataset optimization: critical for resource management"
    },
    "numba_optimization": {
      "enable_numba": true,
      "warmup_numba": true,
      "parallel_numba": true,
      "cache_numba": true,
      "stability_enhancements": {
        "enable_kernel_warmup": true,
        "warmup_iterations": 2,
        "_warmup_iterations_note": "Large dataset optimization: minimal warmup for performance",
        "optimize_memory_layout": true,
        "enable_nogil": true,
        "environment_optimization": {
          "auto_configure": true,
          "max_threads": 8,
          "_max_threads_note": "Large dataset optimization: moderate threading for 3-parameter efficiency",
          "gc_optimization": true
        }
      },
      "performance_monitoring": {
        "enable_profiling": false,
        "_enable_profiling_note": "Large dataset optimization: disabled for performance",
        "stable_benchmarking": false,
        "adaptive_benchmarking": false,
        "performance_baselines": false,
        "target_cv": 0.2,
        "_target_cv_note": "Large dataset optimization: relaxed CV for performance",
        "memory_monitoring": true,
        "smart_caching": {
          "enabled": true,
          "max_items": 200,
          "_max_items_note": "Large dataset optimization: aggressive caching for performance",
          "max_memory_mb": 1000.0,
          "_max_memory_mb_note": "Large dataset optimization: large cache for efficiency"
        }
      }
    },
    "noise_model": {
      "use_simple_forward_model": true,
      "_note": "Large dataset optimization: true uses simplified model for performance",
      "sigma_prior": 0.2,
      "_sigma_prior_note": "Large dataset optimization: broader sigma prior for efficiency"
    }
  },
  "validation_rules": {
    "_comment": "Validation rules optimized for large dataset static isotropic analysis",
    "data_quality": {
      "check_data_range": true,
      "correlation_minimum": 0.0,
      "correlation_maximum": 10.0,
      "check_nan_values": true,
      "nan_handling": "raise"
    },
    "parameter_validation": {
      "check_bounds": true,
      "physics_constraints": true,
      "correlation_checks": true
    },
    "fit_quality": {
      "_comment": "Quality thresholds for 3-parameter optimization - leveraging large dataset statistical power",
      "overall_chi_squared": {
        "excellent_threshold": 8.0,
        "_excellent_threshold_note": "Large dataset optimization: relaxed thresholds leveraging statistical robustness",
        "acceptable_threshold": 15.0,
        "warning_threshold": 30.0,
        "critical_threshold": 60.0
      },
      "per_angle_chi_squared": {
        "excellent_threshold": 8.0,
        "acceptable_threshold": 15.0,
        "warning_threshold": 30.0,
        "outlier_threshold_multiplier": 3.0,
        "_outlier_threshold_multiplier_note": "Large dataset optimization: relaxed outlier detection",
        "max_outlier_fraction": 0.35,
        "_max_outlier_fraction_note": "Large dataset optimization: higher outlier tolerance",
        "min_good_angles": 1,
        "_min_good_angles_note": "Large dataset optimization: relaxed angle requirements for isotropic mode"
      }
    },
    "mcmc_convergence": {
      "_comment": "MCMC convergence diagnostics - leveraging large dataset statistical power",
      "rhat_thresholds": {
        "excellent_threshold": 1.02,
        "good_threshold": 1.08,
        "acceptable_threshold": 1.15,
        "critical_threshold": 1.25,
        "_note": "Large dataset optimization: relaxed R-hat thresholds leveraging convergence robustness"
      },
      "ess_thresholds": {
        "excellent_threshold": 200,
        "good_threshold": 100,
        "acceptable_threshold": 50,
        "minimum_threshold": 25,
        "_note": "Large dataset optimization: reduced ESS requirements leveraging statistical power"
      },
      "divergence_thresholds": {
        "max_divergences_fraction": 0.1,
        "warning_divergences_fraction": 0.02,
        "_note": "Large dataset optimization: higher divergence tolerance for efficiency"
      }
    },
    "frame_range": {
      "minimum_frames": 100000,
      "_minimum_frames_note": "Large dataset optimization: high minimum for true large dataset analysis",
      "maximum_frames": 50000000,
      "_maximum_frames_note": "Large dataset optimization: very large dataset upper limit (50M)",
      "check_continuity": true
    }
  },
  "workflow_integration": {
    "_comment": "Analysis workflow settings optimized for large datasets",
    "analysis_workflow": {
      "auto_generate_plots": true,
      "plot_integration_enabled": true,
      "plot_experimental_data_on_load": false,
      "cache_plot_data": true,
      "save_intermediate_plots": false,
      "_save_intermediate_plots_note": "Large dataset optimization: streamlined plotting for memory efficiency"
    },
    "mcmc_integration": {
      "auto_save_traces": true,
      "trace_file_format": "netcdf",
      "include_warmup_in_traces": false,
      "_include_warmup_in_traces_note": "Large dataset optimization: exclude warmup for storage efficiency",
      "convergence_diagnostics_auto": true,
      "plot_mcmc_results": true
    },
    "data_management": {
      "experimental_data_cache": true,
      "theoretical_data_cache": true,
      "cache_directory": "./cache",
      "auto_cleanup_cache": true,
      "_auto_cleanup_cache_note": "Large dataset optimization: enabled for storage management",
      "cache_retention_days": 14,
      "_cache_retention_days_note": "Large dataset optimization: reduced retention for storage efficiency"
    },
    "error_handling": {
      "continue_on_plot_errors": true,
      "log_plot_errors": true,
      "fallback_plotting": true,
      "validate_plot_data": true
    }
  },
  "output_settings": {
    "_comment": "Output and reporting configuration optimized for large dataset analysis",
    "_output_structure": "homodyne_analysis_results.json (main summary) saved to output directory root. Method-specific results saved to individual directories: classical/[method_name]/ and robust/[method_name]/ containing analysis_results_[method_name].json, parameters.json, fitted_data.npz (consolidated: c2_experimental, c2_fitted, residuals, parameters, uncertainties, chi_squared, phi_angles, t1, t2), and c2_heatmaps_[method_name].png. Summary files: all_classical_methods_summary.json and all_robust_methods_summary.json. MCMC: results saved to mcmc/ subdirectory with same fitted_data.npz structure.",
    "results_directory": "./homodyne_results_large_dataset",
    "_results_directory_note": "Large dataset optimization: dedicated directory for large dataset results",
    "file_formats": {
      "results_format": "json",
      "save_intermediate": false,
      "_save_intermediate_note": "Large dataset optimization: disabled for storage efficiency",
      "compression": true,
      "_compression_note": "Large dataset optimization: enabled for storage optimization",
      "precision": "float32",
      "_precision_note": "Large dataset optimization: memory-efficient precision"
    },
    "file_naming": {
      "timestamp_format": "%Y%m%d_%H%M%S",
      "include_config_name": true,
      "include_chi_squared": true
    },
    "reporting": {
      "generate_plots": true,
      "plot_formats": [
        "png"
      ],
      "_plot_formats_note": "Large dataset optimization: single format for efficiency",
      "detailed_summary": false,
      "_detailed_summary_note": "Large dataset optimization: streamlined summary for performance",
      "convergence_diagnostics": true
    },
    "plotting": {
      "_comment": "Plotting configuration for static isotropic mode with large dataset enhancements",
      "general": {
        "create_plots": true,
        "plot_format": "png",
        "dpi": 150,
        "_dpi_note": "Large dataset optimization: reduced DPI for storage efficiency",
        "figure_size": [
          10,
          8
        ],
        "_figure_size_note": "Standard figures for large dataset memory efficiency",
        "style": "publication",
        "save_plots": true,
        "show_plots": false
      },
      "c2_heatmaps": {
        "enabled": true,
        "_method_specific_note": "When multiple optimization methods are used (e.g., Nelder-Mead + Gurobi), separate heatmaps are generated for each method with method names in filenames",
        "layout": "single_row",
        "include_experimental": true,
        "include_theoretical": true,
        "include_residuals": true,
        "colormap": "viridis",
        "colorbar_position": "right",
        "title_prefix": "C2 Correlation Function (Large Dataset Static Isotropic)",
        "figsize": [
          12,
          4
        ],
        "_figsize_note": "Large dataset optimization: standard figure sizes for memory efficiency"
      },
      "mcmc_plots": {
        "enabled": true,
        "corner_plots": {
          "enabled": true,
          "show_titles": true,
          "quantiles": [
            0.16,
            0.5,
            0.84
          ],
          "show_truths": false,
          "use_arviz": true,
          "figsize": [
            9,
            9
          ],
          "_figsize_note": "Large dataset optimization: compact size for 3 parameters with efficiency"
        },
        "trace_plots": {
          "enabled": true,
          "show_chains": true,
          "show_warmup": false,
          "_show_warmup_note": "Large dataset optimization: exclude warmup for efficiency",
          "compact_layout": true,
          "_compact_layout_note": "Large dataset optimization: compact layout for memory efficiency",
          "figsize": [
            10,
            6
          ],
          "_figsize_note": "Large dataset optimization: compact size for performance"
        },
        "convergence_diagnostics": {
          "enabled": true,
          "show_rhat": true,
          "show_ess": true,
          "show_mcse": false,
          "_show_mcse_note": "Large dataset optimization: disabled for efficiency",
          "show_energy": true,
          "rhat_threshold": 1.15,
          "_rhat_threshold_note": "Large dataset optimization: relaxed threshold for efficiency",
          "ess_threshold": 50,
          "_ess_threshold_note": "Large dataset optimization: lower threshold for speed"
        }
      },
      "diagnostic_plots": {
        "enabled": true,
        "chi_squared_summary": true,
        "parameter_correlations": true,
        "residual_analysis": true,
        "convergence_history": true
      },
      "output": {
        "base_directory": "./plots",
        "subdirectories": {
          "c2_heatmaps": "c2_correlation",
          "parameter_plots": "parameters",
          "mcmc_plots": "mcmc_analysis",
          "diagnostics": "diagnostics"
        },
        "filename_template": "large_dataset_{analysis_type}_{start_frame}_{end_frame}_{method}_{timestamp}",
        "_filename_template_note": "Large dataset optimization: enhanced naming for identification and efficiency",
        "include_timestamp": true,
        "overwrite_existing": true,
        "_overwrite_existing_note": "Large dataset optimization: enabled for storage management"
      }
    },
    "logging": {
      "log_level": "INFO",
      "log_to_file": true,
      "log_to_console": true,
      "log_filename": "homodyne_large_dataset_static_isotropic.log",
      "_log_filename_note": "Large dataset optimization: mode-specific log file for analysis tracking",
      "rotation": {
        "max_bytes": 10485760,
        "backup_count": 2,
        "_backup_count_note": "Large dataset optimization: minimal backup files for storage efficiency"
      }
    }
  }
}