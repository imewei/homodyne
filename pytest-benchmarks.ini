[tool:pytest]
# Benchmark and performance test configuration
# This configuration is specifically for running performance benchmarks and slow tests
# Usage: pytest -c pytest-benchmarks.ini

testpaths = homodyne/tests/performance homodyne/tests/mcmc homodyne/tests/unit/core homodyne/tests/unit/analysis homodyne/tests/unit/optimization
addopts =
    --strict-config
    --strict-markers
    --tb=short
    --maxfail=10
    -v
    --durations=50
    --benchmark-only
    --benchmark-sort=mean
    --benchmark-autosave
    --benchmark-histogram
    --benchmark-storage=file://.benchmarks
    -m "benchmark or performance or slow or hybrid_irls"
    --timeout=1200
    -rA
    --no-header

# Include all performance-related test directories
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Custom markers for benchmarks
markers =
    # Execution time markers
    slow: marks tests as slow (>1s execution time)
    fast: marks tests that run quickly (< 1s)
    
    # Test type markers
    unit: marks unit tests (isolated, no external dependencies)
    integration: marks tests as integration tests
    system: marks system-level tests (require environment setup)
    regression: marks tests for performance regression detection
    
    # Feature-specific markers
    mcmc: marks tests requiring MCMC dependencies
    mcmc_integration: marks tests for MCMC cross-backend integration
    performance: marks performance-related tests
    benchmark: marks tests for benchmarking (requires pytest-benchmark)
    memory: marks tests that monitor memory usage
    import_time: marks tests that measure import performance
    optimization: marks tests for optimization methods (classical, robust, MCMC)
    jit_compilation: marks tests for enhanced JIT compilation optimization
    batch_processing: marks tests for batch processing optimization
    cvxpy_optimization: marks tests for CVXPY solver optimization
    configuration_validation: marks tests for configuration validation features
    
    # Computational method markers
    irls: marks tests related to IRLS variance estimation and convergence
    hybrid_irls: marks tests specifically for hybrid limited-iteration IRLS approach
    weighted_refit: marks tests for weighted refit functionality
    mad_estimation: marks tests for MAD (Median Absolute Deviation) estimation
    vectorized: marks tests for vectorized/optimized computation methods
    numba: marks tests requiring Numba JIT compilation
    convergence: marks tests for iterative algorithm convergence behavior
    angle_filtering: marks tests for angle filtering optimization features
    
    # Platform/dependency markers
    jax: marks tests requiring JAX dependencies
    gpu: marks tests that can utilize GPU acceleration (requires appropriate JAX installation)
    
    # Environment markers
    ci_skip: marks tests to skip in CI environments
    ci: marks tests suitable for CI execution (unit and regression tests)

# Benchmark-specific settings
[tool:pytest-benchmark]
histogram = true
sort = mean
min_rounds = 5
max_time = 5.0
min_time = 0.000005
warmup = true
warmup_iterations = 10000
calibration_precision = 10
disable_gc = false
timer = time.perf_counter

# Skip patterns - focus only on benchmark and performance tests
collect_ignore = homodyne/tests/regression homodyne/tests/integration/test_unified_integration.py homodyne/tests/integration/test_isotropic_mode_integration.py

# Extended timeout for long-running benchmarks (20 minutes)
timeout = 1200

# Performance test environment settings
env_vars =
    NUMBA_NUM_THREADS = 1
    OMP_NUM_THREADS = 1
    OPENBLAS_NUM_THREADS = 1
    MKL_NUM_THREADS = 1